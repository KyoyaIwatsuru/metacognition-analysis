{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Import other libraries\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Import our own libraries\n",
    "sys.path.append(\"../lib/\")\n",
    "import eyegaze as eg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# データパスの設定\n",
    "group_letter = \"A\"  # グループレター: \"A\" または \"B\"\n",
    "participant_id = \"P001\"\n",
    "phase = \"pre\"\n",
    "base_dir = f\"../../data/input/{group_letter}/{participant_id}/{phase}\"\n",
    "\n",
    "# 自動検出: eye_trackingディレクトリ内のタイムスタンプフォルダ（ソートして最新を取得）\n",
    "eye_tracking_base = os.path.join(base_dir, \"eye_tracking\")\n",
    "timestamp_dirs = sorted([d for d in os.listdir(eye_tracking_base) \n",
    "                         if os.path.isdir(os.path.join(eye_tracking_base, d))])\n",
    "eye_tracking_dir = os.path.join(eye_tracking_base, timestamp_dirs[-1])  # 最新\n",
    "\n",
    "# イベントログ（ソートして最新を取得）\n",
    "log_dir = os.path.join(base_dir, \"logs\")\n",
    "event_files = sorted([f for f in os.listdir(log_dir) if f.endswith('.jsonl')])\n",
    "event_log_path = os.path.join(log_dir, event_files[-1])  # 最新\n",
    "\n",
    "# 出力先\n",
    "output_base = f\"../../data/output/{group_letter}/{participant_id}/{phase}\"\n",
    "working_base = f\"../../data/working/{group_letter}/{participant_id}/{phase}\"\n",
    "\n",
    "print(f\"Group: {group_letter}, Participant: {participant_id}, Phase: {phase}\")\n",
    "print(f\"Eye tracking dir: {eye_tracking_dir}\")\n",
    "print(f\"Event log: {event_log_path}\")\n",
    "print(f\"Output: {output_base}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 頭部位置補正の設定 ==========\n",
    "APPLY_HEAD_CORRECTION = False     # 頭部位置補正を適用するか\n",
    "CORRECTION_METHOD = \"trackbox\"   # \"geometric\" または \"trackbox\"\n",
    "\n",
    "# --- 共通設定 ---\n",
    "SCREEN_WIDTH_PX = 1920\n",
    "SCREEN_HEIGHT_PX = 1080\n",
    "\n",
    "# --- geometric方式用 ---\n",
    "USE_AVERAGE_REFERENCE = True\n",
    "CORRECT_Y = True\n",
    "CALIBRATION_HEAD_X = 0.0\n",
    "CALIBRATION_HEAD_Y = 0.0\n",
    "SCREEN_WIDTH_MM = 509.0\n",
    "SCREEN_HEIGHT_MM = 287.0\n",
    "\n",
    "# --- trackbox方式用 ---\n",
    "CORRECTION_FACTOR_X = 500.0\n",
    "CORRECTION_FACTOR_Y = -100.0\n",
    "CALIBRATION_CENTER_X = 0.5\n",
    "CALIBRATION_CENTER_Y = 0.5\n",
    "# ==========================================\n",
    "\n",
    "# データ読み込み（統合API - phaseで切り替え）\n",
    "# 使用可能なフェーズ: \"pre\", \"posttest\", \"training1\", \"training2\"\n",
    "segments = eg.readTobiiData(\n",
    "    eye_tracking_dir=eye_tracking_dir,\n",
    "    event_log_path=event_log_path,\n",
    "    phase=phase,\n",
    "    apply_head_correction=APPLY_HEAD_CORRECTION,\n",
    "    correction_method=CORRECTION_METHOD,\n",
    "    # geometric用\n",
    "    use_average_reference=USE_AVERAGE_REFERENCE,\n",
    "    correct_y=CORRECT_Y,\n",
    "    calibration_head_x=CALIBRATION_HEAD_X,\n",
    "    calibration_head_y=CALIBRATION_HEAD_Y,\n",
    "    screen_width_mm=SCREEN_WIDTH_MM,\n",
    "    screen_height_mm=SCREEN_HEIGHT_MM,\n",
    "    screen_width_px=SCREEN_WIDTH_PX,\n",
    "    screen_height_px=SCREEN_HEIGHT_PX,\n",
    "    # trackbox用\n",
    "    correction_factor_x=CORRECTION_FACTOR_X,\n",
    "    correction_factor_y=CORRECTION_FACTOR_Y,\n",
    "    calibration_center_x=CALIBRATION_CENTER_X,\n",
    "    calibration_center_y=CALIBRATION_CENTER_Y\n",
    ")\n",
    "\n",
    "correction_str = f\"{CORRECTION_METHOD}\" if APPLY_HEAD_CORRECTION else \"none\"\n",
    "print(f\"Total segments: {len(segments)} (head correction: {correction_str})\")\n",
    "for seg in segments:\n",
    "    # フェーズに応じた表示\n",
    "    if 'event_type' in seg:\n",
    "        print(f\"  {seg['image_number']}: {seg['event_type']:30s} | {len(seg['data']):5d} samples, {seg['duration']:.2f}s\")\n",
    "    else:\n",
    "        print(f\"  {seg['passage_id']}: {len(seg['data'])} samples, {seg['duration']:.2f}s, image: {seg['image_number']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 頭部位置補正の比較表示（補正なしデータを読み込んで比較）\n",
    "if APPLY_HEAD_CORRECTION:\n",
    "    segments_original = eg.readTobiiData(\n",
    "        eye_tracking_dir=eye_tracking_dir,\n",
    "        event_log_path=event_log_path,\n",
    "        phase=phase,\n",
    "        apply_head_correction=False  # 補正なし\n",
    "    )\n",
    "    \n",
    "    # サンプルセグメントで比較表示\n",
    "    sample_idx = 0\n",
    "    original_data = segments_original[sample_idx]['data']\n",
    "    corrected_data = segments[sample_idx]['data']\n",
    "    image_path = segments[sample_idx]['image_path']\n",
    "    \n",
    "    print(f\"Comparing: {segments[sample_idx].get('passage_id') or segments[sample_idx].get('event_type')}\")\n",
    "    print(f\"Correction method: {CORRECTION_METHOD}\")\n",
    "    print(f\"Original gaze_x range: {original_data[:, 1].min():.1f} - {original_data[:, 1].max():.1f}\")\n",
    "    print(f\"Corrected gaze_x range: {corrected_data[:, 1].min():.1f} - {corrected_data[:, 1].max():.1f}\")\n",
    "    print(f\"X shift (mean): {corrected_data[:, 1].mean() - original_data[:, 1].mean():.1f} px\")\n",
    "    print(f\"Y shift (mean): {corrected_data[:, 2].mean() - original_data[:, 2].mean():.1f} px\")\n",
    "    \n",
    "    eg.plotGazeCorrectionComparison(\n",
    "        original_data, corrected_data, \n",
    "        bg_image=image_path,\n",
    "        title=f\"Head Position Correction ({CORRECTION_METHOD})\"\n",
    "    )\n",
    "else:\n",
    "    print(\"Head correction is disabled. Set APPLY_HEAD_CORRECTION=True to see comparison.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# サンプル可視化（1つのセグメント）\n",
    "sample_seg = segments[0]\n",
    "data = sample_seg['data']\n",
    "image_path = sample_seg['image_path']\n",
    "\n",
    "print(f\"Analyzing: {sample_seg['passage_id']}\")\n",
    "\n",
    "# 生の視線データ（スキャンパス）\n",
    "eg.plotScanPath(data[:, 1], data[:, 2], \n",
    "                np.array([50.0 for x in data]), \n",
    "                bg_image=image_path)\n",
    "\n",
    "# Fixation検出（瞳孔径も含める）\n",
    "fx = eg.detectFixations(data[:, 0], data[:, 1], data[:, 2], P=data[:, 3],\n",
    "                        min_concat_gaze_count=9, \n",
    "                        min_fixation_size=20, \n",
    "                        max_fixation_size=40)\n",
    "\n",
    "# Fixationベースのスキャンパス（秒単位なのでduration_scale=1000）\n",
    "eg.plotScanPath(fx[:, 1], fx[:, 2], fx[:, 3], \n",
    "                bg_image=image_path, duration_scale=1000)\n",
    "\n",
    "# ヒートマップ\n",
    "eg.plotHeatmap(fx[:, 1], fx[:, 2], fx[:, 3], \n",
    "               bg_image=image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全セグメントの一括処理\n",
    "for segment in segments:\n",
    "    passage_id = segment['passage_id']\n",
    "    img_num = segment['image_number']\n",
    "    data = segment['data']\n",
    "    image_path = segment['image_path']\n",
    "    \n",
    "    print(f\"Processing {passage_id} (image {img_num})...\")\n",
    "    \n",
    "    # 出力ディレクトリ作成\n",
    "    for subdir in ['gaze_raw', 'scan_path', 'heatmap']:\n",
    "        os.makedirs(os.path.join(output_base, subdir), exist_ok=True)\n",
    "    os.makedirs(os.path.join(working_base, 'fixation'), exist_ok=True)\n",
    "    \n",
    "    # 生の視線データ\n",
    "    eg.plotScanPath(\n",
    "        data[:, 1], data[:, 2], \n",
    "        np.array([50.0 for x in data]),\n",
    "        bg_image=image_path, \n",
    "        save_path=os.path.join(output_base, 'gaze_raw', f\"{img_num}.png\")\n",
    "    )\n",
    "    \n",
    "    # Fixation検出（瞳孔径も含める）\n",
    "    fx = eg.detectFixations(data[:, 0], data[:, 1], data[:, 2], P=data[:, 3],\n",
    "                            min_concat_gaze_count=9,\n",
    "                            min_fixation_size=20,\n",
    "                            max_fixation_size=40)\n",
    "    \n",
    "    # 注視点が検出されなかった場合はスキップ\n",
    "    if fx.shape[0] == 0:\n",
    "        print(f\"  Skipped: {img_num} (no fixations detected)\")\n",
    "        continue\n",
    "    \n",
    "    # Fixationデータ保存\n",
    "    np.savetxt(\n",
    "        os.path.join(working_base, 'fixation', f\"{img_num}.csv\"),\n",
    "        fx, \n",
    "        delimiter=',', \n",
    "        header='timestamp,x,y,duration,saccade_length,saccade_angle,saccade_speed,pupil_diameter',\n",
    "        comments=''\n",
    "    )\n",
    "    \n",
    "    # スキャンパス（秒単位なのでduration_scale=1000）\n",
    "    eg.plotScanPath(\n",
    "        fx[:, 1], fx[:, 2], fx[:, 3],\n",
    "        bg_image=image_path,\n",
    "        save_path=os.path.join(output_base, 'scan_path', f\"{img_num}.png\"),\n",
    "        duration_scale=1000\n",
    "    )\n",
    "    \n",
    "    # ヒートマップ\n",
    "    eg.plotHeatmap(\n",
    "        fx[:, 1], fx[:, 2], fx[:, 3],\n",
    "        bg_image=image_path,\n",
    "        save_path=os.path.join(output_base, 'heatmap', f\"{img_num}.png\")\n",
    "    )\n",
    "\n",
    "print(\"All segments processed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 統計情報の出力\n",
    "stats = []\n",
    "for segment in segments:\n",
    "    fx = eg.detectFixations(segment['data'][:, 0], \n",
    "                           segment['data'][:, 1], \n",
    "                           segment['data'][:, 2],\n",
    "                           P=segment['data'][:, 3],\n",
    "                           min_concat_gaze_count=9,\n",
    "                           min_fixation_size=50,\n",
    "                           max_fixation_size=80)\n",
    "    \n",
    "    stat = {\n",
    "        'image_number': segment['image_number'],\n",
    "        'duration_sec': segment['duration'],\n",
    "        'raw_samples': len(segment['data']),\n",
    "        'fixation_count': len(fx),\n",
    "        'total_fixation_duration': fx[:, 3].sum() if len(fx) > 0 else 0,\n",
    "        'mean_fixation_duration': fx[:, 3].mean() if len(fx) > 0 else 0,\n",
    "        'mean_pupil_diameter': fx[:, 7].mean() if len(fx) > 0 else 0\n",
    "    }\n",
    "    \n",
    "    # フェーズに応じて追加フィールド\n",
    "    if 'passage_id' in segment:\n",
    "        stat['passage_id'] = segment['passage_id']\n",
    "    if 'event_type' in segment:\n",
    "        stat['event_type'] = segment['event_type']\n",
    "    if 'analog_id' in segment:\n",
    "        stat['analog_id'] = segment['analog_id']\n",
    "    \n",
    "    stats.append(stat)\n",
    "\n",
    "stats_df = pd.DataFrame(stats)\n",
    "print(stats_df)\n",
    "\n",
    "# CSV保存\n",
    "stats_df.to_csv(os.path.join(output_base, 'statistics.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-science-intro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
