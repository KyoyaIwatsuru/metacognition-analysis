{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A群 vs B群 視線行動比較分析\n",
    "\n",
    "メタ認知支援解説（B群）が視線行動に与える影響を多角的に分析し、群間差を明らかにする。\n",
    "\n",
    "- **セクションA**: セグメントレベル分析（statistics.csv活用）\n",
    "- **セクションB**: トレーニング進行分析\n",
    "- **セクションC**: AOIレベル分析（注視配分）\n",
    "- **セクションD**: 探索的分析\n",
    "- **セクションE**: 統合的統計まとめ\n",
    "- **セクションF**: 結論・出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy.stats import nct\n",
    "from scipy.integrate import quad\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams['font.family'] = 'Hiragino Sans'\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "%matplotlib inline\n",
    "\n",
    "sys.path.append('../lib/')\n",
    "import eyegaze as eg\n",
    "\n",
    "# パス定義\n",
    "PROJECT_ROOT = Path.cwd().parent.parent\n",
    "INPUT_ROOT = PROJECT_ROOT / 'data' / 'input'\n",
    "WORKING_ROOT = PROJECT_ROOT / 'data' / 'working'\n",
    "OUTPUT_ROOT = PROJECT_ROOT / 'data' / 'output'\n",
    "\n",
    "# 参加者定義\n",
    "# P005, P011: pre-testの視線データ品質不良のため除外\n",
    "#   P005: 有効視線率 24.4%（右目がほぼ未計測）\n",
    "#   P011: 有効視線率 43.0%（両目とも不良）\n",
    "participants = {\n",
    "    'A': ['P001', 'P002', 'P006', 'P008', 'P009', 'P010', 'P016', 'P017'],\n",
    "    'B': ['P003', 'P004', 'P007', 'P012', 'P013', 'P014', 'P015', 'P018', 'P019', 'P020'],\n",
    "}\n",
    "phases = ['pre', 'post', 'training1', 'training2', 'training3']\n",
    "TOLERANCE = 5.0\n",
    "\n",
    "print(f'Project root: {PROJECT_ROOT}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## セクションA: セグメントレベル分析（statistics.csv活用）\n",
    "\n",
    "全参加者の `statistics.csv` を集約し、読解時間・固視回数・瞳孔径等のセグメントレベル指標を群間比較する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- A-1: statistics.csv の集約 ---\n",
    "all_stats = []\n",
    "\n",
    "for group, pids in participants.items():\n",
    "    for pid in pids:\n",
    "        for phase in phases:\n",
    "            csv_path = OUTPUT_ROOT / group / pid / phase / 'statistics.csv'\n",
    "            if not csv_path.exists():\n",
    "                continue\n",
    "            df_tmp = pd.read_csv(csv_path)\n",
    "            df_tmp['group'] = group\n",
    "            df_tmp['participant'] = pid\n",
    "            df_tmp['phase'] = phase\n",
    "            all_stats.append(df_tmp)\n",
    "\n",
    "df_all = pd.concat(all_stats, ignore_index=True)\n",
    "print(f'全レコード数: {len(df_all)}')\n",
    "print(f'カラム: {list(df_all.columns)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- A-2: 問題画面のみフィルタ ---\n",
    "# pre/post: question_screen_open\n",
    "# training: question_screen_open + analog_question_open_an{1,2,3}\n",
    "question_events = ['question_screen_open',\n",
    "                   'analog_question_open_an1', 'analog_question_open_an2', 'analog_question_open_an3']\n",
    "\n",
    "df_q = df_all[df_all['event_type'].isin(question_events)].copy()\n",
    "print(f'問題画面レコード数: {len(df_q)}')\n",
    "print(df_q.groupby(['group', 'phase', 'event_type']).size().unstack(fill_value=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- A-3: 参加者×フェーズごとに集約 ---\n",
    "seg_metrics = [\n",
    "    # 基本\n",
    "    'duration_sec',\n",
    "    # 固視\n",
    "    'fixation_count', 'fixation_rate',\n",
    "    'total_fixation_duration', 'mean_fixation_duration', 'std_fixation_duration',\n",
    "    # 瞳孔\n",
    "    'mean_pupil_diameter', 'std_pupil_diameter',\n",
    "    # サッカード\n",
    "    'mean_saccade_length', 'std_saccade_length',\n",
    "    'mean_saccade_speed', 'std_saccade_speed',\n",
    "    'regression_rate',\n",
    "]\n",
    "\n",
    "seg_metric_labels = {\n",
    "    'duration_sec': '読解時間 (秒)',\n",
    "    'fixation_count': '固視回数',\n",
    "    'fixation_rate': '固視率 (回/秒)',\n",
    "    'total_fixation_duration': '総固視時間 (秒)',\n",
    "    'mean_fixation_duration': '平均固視時間 (秒)',\n",
    "    'std_fixation_duration': '固視時間SD (秒)',\n",
    "    'mean_pupil_diameter': '平均瞳孔径 (mm)',\n",
    "    'std_pupil_diameter': '瞳孔径SD (mm)',\n",
    "    'mean_saccade_length': '平均サッカード長 (px)',\n",
    "    'std_saccade_length': 'サッカード長SD (px)',\n",
    "    'mean_saccade_speed': '平均サッカード速度 (px/s)',\n",
    "    'std_saccade_speed': 'サッカード速度SD (px/s)',\n",
    "    'regression_rate': '回帰率',\n",
    "}\n",
    "\n",
    "df_seg = df_q.groupby(['group', 'participant', 'phase'])[seg_metrics].mean().reset_index()\n",
    "print(f'参加者レベルDF: {df_seg.shape}')\n",
    "df_seg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- A-4: 記述統計 ---\n",
    "print('【セグメントレベル記述統計】')\n",
    "print('=' * 70)\n",
    "\n",
    "for metric in seg_metrics:\n",
    "    print(f'\\n--- {seg_metric_labels[metric]} ---')\n",
    "    summary = df_seg.groupby(['group', 'phase'])[metric].agg(['mean', 'std', 'count'])\n",
    "    summary.columns = ['M', 'SD', 'n']\n",
    "    summary['M(SD)'] = summary.apply(lambda r: f\"{r['M']:.3f} ({r['SD']:.3f})\", axis=1)\n",
    "    pivot = summary['M(SD)'].unstack(level='phase')\n",
    "    phase_order = [p for p in ['pre', 'training1', 'training2', 'training3', 'post'] if p in pivot.columns]\n",
    "    print(pivot[phase_order])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- A-5: ヘルパー関数群 ---\n",
    "\n",
    "def get_paired_data(df, group, metric, phase_pre='pre', phase_post='post'):\n",
    "    \"\"\"群内のpre/postペアデータを取得\"\"\"\n",
    "    pre = df[(df['group'] == group) & (df['phase'] == phase_pre)].set_index('participant')[metric]\n",
    "    post = df[(df['group'] == group) & (df['phase'] == phase_post)].set_index('participant')[metric]\n",
    "    common = pre.index.intersection(post.index)\n",
    "    pre_v = pre.loc[common].dropna()\n",
    "    post_v = post.loc[common].dropna()\n",
    "    common2 = pre_v.index.intersection(post_v.index)\n",
    "    return pre_v.loc[common2].values, post_v.loc[common2].values\n",
    "\n",
    "\n",
    "def cohens_d_paired(pre, post):\n",
    "    diff = post - pre\n",
    "    sd = np.std(diff, ddof=1)\n",
    "    return np.mean(diff) / sd if sd > 0 else 0.0\n",
    "\n",
    "\n",
    "def cohens_d_ind(x, y):\n",
    "    nx, ny = len(x), len(y)\n",
    "    pooled = np.sqrt(((nx-1)*np.var(x, ddof=1) + (ny-1)*np.var(y, ddof=1)) / (nx+ny-2))\n",
    "    return (np.mean(x) - np.mean(y)) / pooled if pooled > 0 else 0.0\n",
    "\n",
    "\n",
    "def bootstrap_ci(x, y, func, n_boot=10000, ci=95, seed=42):\n",
    "    \"\"\"ブートストラップ信頼区間\"\"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "    stats_boot = []\n",
    "    for _ in range(n_boot):\n",
    "        bx = x[rng.choice(len(x), len(x), replace=True)]\n",
    "        by = y[rng.choice(len(y), len(y), replace=True)]\n",
    "        stats_boot.append(func(bx, by))\n",
    "    lo = np.percentile(stats_boot, (100 - ci) / 2)\n",
    "    hi = np.percentile(stats_boot, 100 - (100 - ci) / 2)\n",
    "    return lo, hi\n",
    "\n",
    "\n",
    "def mixed_anova(pre_A, post_A, pre_B, post_B):\n",
    "    \"\"\"2x2 Mixed ANOVA: Group(A/B) x Time(pre/post)\"\"\"\n",
    "    n_A, n_B = len(pre_A), len(pre_B)\n",
    "    N = n_A + n_B\n",
    "    grand_mean = np.mean(np.concatenate([pre_A, post_A, pre_B, post_B]))\n",
    "    mean_A = np.mean(np.concatenate([pre_A, post_A]))\n",
    "    mean_B = np.mean(np.concatenate([pre_B, post_B]))\n",
    "    mean_pre = np.mean(np.concatenate([pre_A, pre_B]))\n",
    "    mean_post = np.mean(np.concatenate([post_A, post_B]))\n",
    "    mean_A_pre, mean_A_post = np.mean(pre_A), np.mean(post_A)\n",
    "    mean_B_pre, mean_B_post = np.mean(pre_B), np.mean(post_B)\n",
    "\n",
    "    SS_group = 2 * (n_A * (mean_A - grand_mean)**2 + n_B * (mean_B - grand_mean)**2)\n",
    "    SS_time = N * ((mean_pre - grand_mean)**2 + (mean_post - grand_mean)**2)\n",
    "    SS_ix = (n_A * ((mean_A_pre - mean_A - mean_pre + grand_mean)**2 +\n",
    "                    (mean_A_post - mean_A - mean_post + grand_mean)**2) +\n",
    "             n_B * ((mean_B_pre - mean_B - mean_pre + grand_mean)**2 +\n",
    "                    (mean_B_post - mean_B - mean_post + grand_mean)**2))\n",
    "\n",
    "    subj_A = (pre_A + post_A) / 2\n",
    "    subj_B = (pre_B + post_B) / 2\n",
    "    SS_subj = 2 * (np.sum((subj_A - mean_A)**2) + np.sum((subj_B - mean_B)**2))\n",
    "\n",
    "    SS_ew = 0.0\n",
    "    for i in range(n_A):\n",
    "        SS_ew += (pre_A[i] - subj_A[i] - mean_pre + mean_A)**2\n",
    "        SS_ew += (post_A[i] - subj_A[i] - mean_post + mean_A)**2\n",
    "    for i in range(n_B):\n",
    "        SS_ew += (pre_B[i] - subj_B[i] - mean_pre + mean_B)**2\n",
    "        SS_ew += (post_B[i] - subj_B[i] - mean_post + mean_B)**2\n",
    "\n",
    "    df_g, df_t, df_ix = 1, 1, 1\n",
    "    df_s = N - 2\n",
    "    df_ew = N - 2\n",
    "\n",
    "    MS_g = SS_group / df_g\n",
    "    MS_t = SS_time / df_t\n",
    "    MS_ix = SS_ix / df_ix\n",
    "    MS_s = SS_subj / df_s if df_s > 0 else 1e-10\n",
    "    MS_ew = SS_ew / df_ew if df_ew > 0 else 1e-10\n",
    "\n",
    "    F_g = MS_g / MS_s\n",
    "    F_t = MS_t / MS_ew\n",
    "    F_ix = MS_ix / MS_ew\n",
    "\n",
    "    p_g = 1 - stats.f.cdf(F_g, df_g, df_s)\n",
    "    p_t = 1 - stats.f.cdf(F_t, df_t, df_ew)\n",
    "    p_ix = 1 - stats.f.cdf(F_ix, df_ix, df_ew)\n",
    "\n",
    "    eta2_g = SS_group / (SS_group + SS_subj) if (SS_group + SS_subj) > 0 else 0\n",
    "    eta2_t = SS_time / (SS_time + SS_ew) if (SS_time + SS_ew) > 0 else 0\n",
    "    eta2_ix = SS_ix / (SS_ix + SS_ew) if (SS_ix + SS_ew) > 0 else 0\n",
    "\n",
    "    return {\n",
    "        'F_group': F_g, 'p_group': p_g, 'eta2_group': eta2_g, 'df_g': df_g, 'df_s': df_s,\n",
    "        'F_time': F_t, 'p_time': p_t, 'eta2_time': eta2_t, 'df_t': df_t, 'df_ew': df_ew,\n",
    "        'F_interaction': F_ix, 'p_interaction': p_ix, 'eta2_interaction': eta2_ix,\n",
    "    }\n",
    "\n",
    "print('ヘルパー関数定義完了')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- A-5b: 正規性検定 (Shapiro-Wilk) ---\n",
    "print('【正規性検定 (Shapiro-Wilk) — セグメントレベル指標】')\n",
    "print('=' * 70)\n",
    "print('※ p > 0.05 で正規性を仮定（○）、p ≤ 0.05 で棄却（×）')\n",
    "\n",
    "normality_rows = []\n",
    "\n",
    "for metric in seg_metrics:\n",
    "    for group in ['A', 'B']:\n",
    "        pre_v, post_v = get_paired_data(df_seg, group, metric)\n",
    "        if len(pre_v) < 3:\n",
    "            continue\n",
    "        diff = post_v - pre_v\n",
    "\n",
    "        # Pre\n",
    "        w_pre, p_pre = stats.shapiro(pre_v)\n",
    "        # Post\n",
    "        w_post, p_post = stats.shapiro(post_v)\n",
    "        # 差分 (post - pre)\n",
    "        w_diff, p_diff = stats.shapiro(diff)\n",
    "\n",
    "        normality_rows.append({\n",
    "            '指標': seg_metric_labels[metric],\n",
    "            '群': group,\n",
    "            'n': len(pre_v),\n",
    "            'Pre W': f'{w_pre:.3f}',\n",
    "            'Pre p': f'{p_pre:.3f}',\n",
    "            'Pre 正規性': '○' if p_pre > 0.05 else '×',\n",
    "            'Post W': f'{w_post:.3f}',\n",
    "            'Post p': f'{p_post:.3f}',\n",
    "            'Post 正規性': '○' if p_post > 0.05 else '×',\n",
    "            '差分 W': f'{w_diff:.3f}',\n",
    "            '差分 p': f'{p_diff:.3f}',\n",
    "            '差分 正規性': '○' if p_diff > 0.05 else '×',\n",
    "        })\n",
    "\n",
    "norm_seg_df = pd.DataFrame(normality_rows)\n",
    "print()\n",
    "display(norm_seg_df)\n",
    "\n",
    "# 群間比較用: ゲインスコアの正規性\n",
    "print('\\n【ゲインスコアの正規性検定（群間比較の前提確認）】')\n",
    "print('-' * 70)\n",
    "\n",
    "gain_norm_rows = []\n",
    "for metric in seg_metrics:\n",
    "    pre_A, post_A = get_paired_data(df_seg, 'A', metric)\n",
    "    pre_B, post_B = get_paired_data(df_seg, 'B', metric)\n",
    "    if len(pre_A) < 3 or len(pre_B) < 3:\n",
    "        continue\n",
    "    gain_A = post_A - pre_A\n",
    "    gain_B = post_B - pre_B\n",
    "\n",
    "    w_A, p_A = stats.shapiro(gain_A)\n",
    "    w_B, p_B = stats.shapiro(gain_B)\n",
    "\n",
    "    gain_norm_rows.append({\n",
    "        '指標': seg_metric_labels[metric],\n",
    "        'A群 W': f'{w_A:.3f}',\n",
    "        'A群 p': f'{p_A:.3f}',\n",
    "        'A群 正規性': '○' if p_A > 0.05 else '×',\n",
    "        'B群 W': f'{w_B:.3f}',\n",
    "        'B群 p': f'{p_B:.3f}',\n",
    "        'B群 正規性': '○' if p_B > 0.05 else '×',\n",
    "    })\n",
    "\n",
    "gain_norm_seg_df = pd.DataFrame(gain_norm_rows)\n",
    "display(gain_norm_seg_df)\n",
    "\n",
    "# 判定サマリー\n",
    "non_normal = norm_seg_df[norm_seg_df['差分 正規性'] == '×']\n",
    "if len(non_normal) > 0:\n",
    "    print(f'\\n⚠ 差分が非正規: {len(non_normal)}件 → 対応ありt検定の代わりにWilcoxon推奨')\n",
    "    for _, row in non_normal.iterrows():\n",
    "        print(f\"  - {row['指標']} ({row['群']}群)\")\n",
    "else:\n",
    "    print('\\n✓ 全指標・全群で差分の正規性が確認された → 対応ありt検定が適用可能')\n",
    "\n",
    "gain_non_normal = gain_norm_seg_df[\n",
    "    (gain_norm_seg_df['A群 正規性'] == '×') | (gain_norm_seg_df['B群 正規性'] == '×')\n",
    "]\n",
    "if len(gain_non_normal) > 0:\n",
    "    print(f'⚠ ゲインが非正規: {len(gain_non_normal)}件 → 独立t検定の代わりにMann-Whitney U推奨')\n",
    "    for _, row in gain_non_normal.iterrows():\n",
    "        flags = []\n",
    "        if row['A群 正規性'] == '×': flags.append('A群')\n",
    "        if row['B群 正規性'] == '×': flags.append('B群')\n",
    "        print(f\"  - {row['指標']} ({', '.join(flags)})\")\n",
    "else:\n",
    "    print('✓ 全指標でゲインスコアの正規性が確認された → 独立t検定が適用可能')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- A-6: 群内 Pre→Post 比較 ---\n",
    "print('【群内 Pre→Post 比較（対応ありt検定 + Wilcoxon）】')\n",
    "print('=' * 70)\n",
    "\n",
    "within_results = []\n",
    "\n",
    "for metric in seg_metrics:\n",
    "    print(f'\\n--- {seg_metric_labels[metric]} ---')\n",
    "    for group in ['A', 'B']:\n",
    "        pre_v, post_v = get_paired_data(df_seg, group, metric)\n",
    "        if len(pre_v) < 3:\n",
    "            print(f'  {group}群: データ不足 (n={len(pre_v)})')\n",
    "            continue\n",
    "        diff = post_v - pre_v\n",
    "        t_stat, t_p = stats.ttest_rel(post_v, pre_v)\n",
    "        try:\n",
    "            w_stat, w_p = stats.wilcoxon(diff)\n",
    "        except ValueError:\n",
    "            w_stat, w_p = np.nan, np.nan\n",
    "        d = cohens_d_paired(pre_v, post_v)\n",
    "\n",
    "        print(f'  {group}群 (n={len(pre_v)}): Pre={np.mean(pre_v):.3f} -> Post={np.mean(post_v):.3f} '\n",
    "              f'(差={np.mean(diff):+.3f}), t={t_stat:.3f}, p={t_p:.3f}, Wilcoxon p={w_p:.3f}, d={d:.3f}')\n",
    "\n",
    "        within_results.append({\n",
    "            '指標': seg_metric_labels[metric], '群': group, 'n': len(pre_v),\n",
    "            'Pre平均': np.mean(pre_v), 'Post平均': np.mean(post_v), '差': np.mean(diff),\n",
    "            't': t_stat, 'p_t': t_p, 'p_wilcoxon': w_p, 'Cohen_d': d,\n",
    "        })\n",
    "\n",
    "within_df = pd.DataFrame(within_results)\n",
    "within_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- A-7: 群間ゲインスコア比較 ---\n",
    "print('【群間ゲインスコア比較（独立t検定 + Mann-Whitney U）】')\n",
    "print('=' * 70)\n",
    "\n",
    "between_results = []\n",
    "\n",
    "for metric in seg_metrics:\n",
    "    print(f'\\n--- {seg_metric_labels[metric]} ---')\n",
    "    pre_A, post_A = get_paired_data(df_seg, 'A', metric)\n",
    "    pre_B, post_B = get_paired_data(df_seg, 'B', metric)\n",
    "    gain_A = post_A - pre_A\n",
    "    gain_B = post_B - pre_B\n",
    "\n",
    "    if len(gain_A) < 3 or len(gain_B) < 3:\n",
    "        print(f'  データ不足')\n",
    "        continue\n",
    "\n",
    "    t_stat, t_p = stats.ttest_ind(gain_A, gain_B)\n",
    "    u_stat, u_p = stats.mannwhitneyu(gain_A, gain_B, alternative='two-sided')\n",
    "    d = cohens_d_ind(gain_B, gain_A)\n",
    "    ci_lo, ci_hi = bootstrap_ci(gain_B, gain_A, cohens_d_ind)\n",
    "\n",
    "    print(f'  A群 ゲイン: M={np.mean(gain_A):+.3f} (SD={np.std(gain_A, ddof=1):.3f})')\n",
    "    print(f'  B群 ゲイン: M={np.mean(gain_B):+.3f} (SD={np.std(gain_B, ddof=1):.3f})')\n",
    "    print(f'  t={t_stat:.3f}, p={t_p:.3f} | U={u_stat:.1f}, p_MW={u_p:.3f} | d={d:.3f} [{ci_lo:.3f}, {ci_hi:.3f}]')\n",
    "\n",
    "    between_results.append({\n",
    "        '指標': seg_metric_labels[metric],\n",
    "        'A群ゲイン平均': np.mean(gain_A), 'A群ゲインSD': np.std(gain_A, ddof=1),\n",
    "        'B群ゲイン平均': np.mean(gain_B), 'B群ゲインSD': np.std(gain_B, ddof=1),\n",
    "        't': t_stat, 'p_t': t_p, 'U': u_stat, 'p_MW': u_p,\n",
    "        'Cohen_d': d, 'CI下限': ci_lo, 'CI上限': ci_hi,\n",
    "    })\n",
    "\n",
    "between_df = pd.DataFrame(between_results)\n",
    "between_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- A-8: 混合分散分析 ---\n",
    "print('【混合分散分析 (群 x 時点)】')\n",
    "print('=' * 70)\n",
    "\n",
    "anova_results = []\n",
    "\n",
    "for metric in seg_metrics:\n",
    "    print(f'\\n--- {seg_metric_labels[metric]} ---')\n",
    "    pre_A, post_A = get_paired_data(df_seg, 'A', metric)\n",
    "    pre_B, post_B = get_paired_data(df_seg, 'B', metric)\n",
    "    if len(pre_A) < 3 or len(pre_B) < 3:\n",
    "        continue\n",
    "\n",
    "    res = mixed_anova(pre_A, post_A, pre_B, post_B)\n",
    "    print(f'  群主効果:   F({res[\"df_g\"]},{res[\"df_s\"]})={res[\"F_group\"]:.3f}, p={res[\"p_group\"]:.3f}, η²p={res[\"eta2_group\"]:.3f}')\n",
    "    print(f'  時点主効果: F({res[\"df_t\"]},{res[\"df_ew\"]})={res[\"F_time\"]:.3f}, p={res[\"p_time\"]:.3f}, η²p={res[\"eta2_time\"]:.3f}')\n",
    "    print(f'  交互作用:   F(1,{res[\"df_ew\"]})={res[\"F_interaction\"]:.3f}, p={res[\"p_interaction\"]:.3f}, η²p={res[\"eta2_interaction\"]:.3f}')\n",
    "\n",
    "    res['指標'] = seg_metric_labels[metric]\n",
    "    anova_results.append(res)\n",
    "\n",
    "anova_df = pd.DataFrame(anova_results)\n",
    "anova_df[['指標', 'F_group', 'p_group', 'eta2_group', 'F_time', 'p_time', 'eta2_time',\n",
    "          'F_interaction', 'p_interaction', 'eta2_interaction']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- A-9: 可視化 Pre-Post 棒グラフ ---\n",
    "fig, axes = plt.subplots(4, 4, figsize=(24, 20))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, metric in enumerate(seg_metrics):\n",
    "    ax = axes[idx]\n",
    "    means, sems, colors = [], [], []\n",
    "    for group, color in [('A', '#4C72B0'), ('B', '#DD8452')]:\n",
    "        for ph in ['pre', 'post']:\n",
    "            vals = df_seg[(df_seg['group'] == group) & (df_seg['phase'] == ph)][metric].dropna().values\n",
    "            means.append(np.mean(vals) if len(vals) > 0 else 0)\n",
    "            sems.append(np.std(vals, ddof=1) / np.sqrt(len(vals)) if len(vals) > 1 else 0)\n",
    "            colors.append(color)\n",
    "    x = np.arange(4)\n",
    "    ax.bar(x, means, yerr=sems, capsize=4, color=colors, alpha=0.8, edgecolor='black', linewidth=0.5)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(['A\\npre', 'A\\npost', 'B\\npre', 'B\\npost'])\n",
    "    ax.set_title(seg_metric_labels[metric])\n",
    "\n",
    "for i in range(len(seg_metrics), len(axes)):\n",
    "    axes[i].set_visible(False)\n",
    "plt.suptitle('セグメントレベル指標: Pre vs Post（群別）', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- A-10: ゲインスコアのボックスプロット ---\n",
    "fig, axes = plt.subplots(4, 4, figsize=(24, 20))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, metric in enumerate(seg_metrics):\n",
    "    ax = axes[idx]\n",
    "    gain_data = []\n",
    "    labels = []\n",
    "    for group in ['A', 'B']:\n",
    "        pre_v, post_v = get_paired_data(df_seg, group, metric)\n",
    "        gains = post_v - pre_v\n",
    "        gain_data.append(gains)\n",
    "        labels.append(f'{group}群\\n(n={len(gains)})')\n",
    "    bp = ax.boxplot(gain_data, tick_labels=labels, patch_artist=True,\n",
    "                    medianprops=dict(color='black', linewidth=2))\n",
    "    bp['boxes'][0].set_facecolor('#4C72B0')\n",
    "    bp['boxes'][0].set_alpha(0.7)\n",
    "    bp['boxes'][1].set_facecolor('#DD8452')\n",
    "    bp['boxes'][1].set_alpha(0.7)\n",
    "    ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "    ax.set_title(f'{seg_metric_labels[metric]}\\n(Post - Pre)')\n",
    "    ax.set_ylabel('ゲイン')\n",
    "\n",
    "for i in range(len(seg_metrics), len(axes)):\n",
    "    axes[i].set_visible(False)\n",
    "plt.suptitle('セグメントレベル: ゲインスコア (Post - Pre)', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## セクションB: トレーニング進行分析\n",
    "\n",
    "トレーニングセット内（本問→類題1→2→3）およびトレーニング間（T1→T2→T3）の進行を群間比較する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- B-1: トレーニングセット内の進行データ構築 ---\n",
    "# 各training内で本問→類題1→類題2→類題3 の4点を取得\n",
    "training_phases = ['training1', 'training2', 'training3']\n",
    "step_events = ['question_screen_open',\n",
    "               'analog_question_open_an1', 'analog_question_open_an2', 'analog_question_open_an3']\n",
    "step_labels = ['本問', '類題1', '類題2', '類題3']\n",
    "\n",
    "intra_rows = []\n",
    "for group, pids in participants.items():\n",
    "    for pid in pids:\n",
    "        for phase in training_phases:\n",
    "            for step_idx, evt in enumerate(step_events):\n",
    "                vals = df_all[(df_all['group'] == group) & (df_all['participant'] == pid) &\n",
    "                              (df_all['phase'] == phase) & (df_all['event_type'] == evt)]\n",
    "                if len(vals) == 0:\n",
    "                    continue\n",
    "                row = vals[seg_metrics].mean()\n",
    "                intra_rows.append({\n",
    "                    'group': group, 'participant': pid, 'phase': phase,\n",
    "                    'step': step_idx, 'step_label': step_labels[step_idx],\n",
    "                    **{m: row[m] for m in seg_metrics}\n",
    "                })\n",
    "\n",
    "df_intra = pd.DataFrame(intra_rows)\n",
    "print(f'セット内進行DF: {df_intra.shape}')\n",
    "df_intra.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- B-2: セット内進行の折れ線グラフ（各trainingを平均） ---\n",
    "fig, axes = plt.subplots(4, 4, figsize=(24, 20))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, metric in enumerate(seg_metrics):\n",
    "    ax = axes[idx]\n",
    "    for group, color, marker in [('A', '#4C72B0', 'o'), ('B', '#DD8452', 's')]:\n",
    "        means, sems = [], []\n",
    "        for step in range(4):\n",
    "            # 全trainingのstepを平均化\n",
    "            vals = df_intra[(df_intra['group'] == group) & (df_intra['step'] == step)]\n",
    "            # 参加者ごとの平均を取ってから群の統計量\n",
    "            pmeans = vals.groupby('participant')[metric].mean().values\n",
    "            means.append(np.mean(pmeans) if len(pmeans) > 0 else np.nan)\n",
    "            sems.append(np.std(pmeans, ddof=1) / np.sqrt(len(pmeans)) if len(pmeans) > 1 else 0)\n",
    "        ax.errorbar(range(4), means, yerr=sems, marker=marker, color=color,\n",
    "                    label=f'{group}群', capsize=4, linewidth=2, markersize=8)\n",
    "    ax.set_xticks(range(4))\n",
    "    ax.set_xticklabels(step_labels, fontsize=9)\n",
    "    ax.set_title(seg_metric_labels[metric])\n",
    "    ax.legend()\n",
    "\n",
    "for i in range(len(seg_metrics), len(axes)):\n",
    "    axes[i].set_visible(False)\n",
    "plt.suptitle('セット内進行 (本問 → 類題1-3)', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- B-3: セット内の傾きを群間比較 ---\n",
    "print('【セット内改善傾き（線形回帰）の群間比較】')\n",
    "print('=' * 70)\n",
    "\n",
    "slope_results = []\n",
    "\n",
    "for metric in seg_metrics:\n",
    "    print(f'\\n--- {seg_metric_labels[metric]} ---')\n",
    "    slopes = {'A': [], 'B': []}\n",
    "    for group, pids in participants.items():\n",
    "        for pid in pids:\n",
    "            p_data = df_intra[(df_intra['group'] == group) & (df_intra['participant'] == pid)]\n",
    "            # 参加者×training ごとの step vs metric の傾き\n",
    "            pmeans = p_data.groupby('step')[metric].mean()\n",
    "            if len(pmeans) >= 3:\n",
    "                from scipy.stats import linregress\n",
    "                sl, _, _, _, _ = linregress(pmeans.index, pmeans.values)\n",
    "                slopes[group].append(sl)\n",
    "\n",
    "    sl_A = np.array(slopes['A'])\n",
    "    sl_B = np.array(slopes['B'])\n",
    "    if len(sl_A) >= 3 and len(sl_B) >= 3:\n",
    "        t_stat, t_p = stats.ttest_ind(sl_A, sl_B)\n",
    "        d = cohens_d_ind(sl_B, sl_A)\n",
    "        print(f'  A群 傾き: M={np.mean(sl_A):.4f} (SD={np.std(sl_A, ddof=1):.4f})')\n",
    "        print(f'  B群 傾き: M={np.mean(sl_B):.4f} (SD={np.std(sl_B, ddof=1):.4f})')\n",
    "        print(f'  t={t_stat:.3f}, p={t_p:.3f}, d={d:.3f}')\n",
    "        slope_results.append({\n",
    "            '指標': seg_metric_labels[metric],\n",
    "            'A群傾き平均': np.mean(sl_A), 'B群傾き平均': np.mean(sl_B),\n",
    "            't': t_stat, 'p': t_p, 'd': d,\n",
    "        })\n",
    "\n",
    "if slope_results:\n",
    "    pd.DataFrame(slope_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- B-4: トレーニング間の進行 (T1→T2→T3 本問のみ) ---\n",
    "print('【トレーニング間の進行（本問メトリクス）】')\n",
    "print('=' * 70)\n",
    "\n",
    "# 本問のみフィルタ\n",
    "df_main_q = df_q[df_q['event_type'] == 'question_screen_open'].copy()\n",
    "df_main_tr = df_main_q[df_main_q['phase'].isin(training_phases)].copy()\n",
    "df_main_tr_agg = df_main_tr.groupby(['group', 'participant', 'phase'])[seg_metrics].mean().reset_index()\n",
    "\n",
    "fig, axes = plt.subplots(4, 4, figsize=(24, 20))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, metric in enumerate(seg_metrics):\n",
    "    ax = axes[idx]\n",
    "    print(f'\\n--- {seg_metric_labels[metric]} ---')\n",
    "\n",
    "    for group, color, marker in [('A', '#4C72B0', 'o'), ('B', '#DD8452', 's')]:\n",
    "        means, sems = [], []\n",
    "        for tp in training_phases:\n",
    "            vals = df_main_tr_agg[(df_main_tr_agg['group'] == group) &\n",
    "                                  (df_main_tr_agg['phase'] == tp)][metric].dropna().values\n",
    "            means.append(np.mean(vals) if len(vals) > 0 else np.nan)\n",
    "            sems.append(np.std(vals, ddof=1) / np.sqrt(len(vals)) if len(vals) > 1 else 0)\n",
    "        ax.errorbar(range(3), means, yerr=sems, marker=marker, color=color,\n",
    "                    label=f'{group}群', capsize=4, linewidth=2, markersize=8)\n",
    "        print(f'  {group}群: ' + ', '.join(f'{tp}={m:.3f}' for tp, m in zip(training_phases, means)))\n",
    "\n",
    "    ax.set_xticks(range(3))\n",
    "    ax.set_xticklabels(training_phases)\n",
    "    ax.set_title(seg_metric_labels[metric])\n",
    "    ax.legend()\n",
    "\n",
    "    # Friedman検定（各群内）\n",
    "    for group in ['A', 'B']:\n",
    "        phase_vals = []\n",
    "        for tp in training_phases:\n",
    "            v = df_main_tr_agg[(df_main_tr_agg['group'] == group) &\n",
    "                               (df_main_tr_agg['phase'] == tp)].set_index('participant')[metric]\n",
    "            phase_vals.append(v)\n",
    "        common = phase_vals[0].index\n",
    "        for pv in phase_vals[1:]:\n",
    "            common = common.intersection(pv.index)\n",
    "        if len(common) >= 3:\n",
    "            arrs = [pv.loc[common].dropna().values for pv in phase_vals]\n",
    "            if all(len(a) == len(arrs[0]) for a in arrs) and len(arrs[0]) >= 3:\n",
    "                chi2, p_f = stats.friedmanchisquare(*arrs)\n",
    "                print(f'  {group}群 Friedman検定: chi2={chi2:.3f}, p={p_f:.3f}')\n",
    "\n",
    "for i in range(len(seg_metrics), len(axes)):\n",
    "    axes[i].set_visible(False)\n",
    "plt.suptitle('トレーニング間の進行（本問のみ: T1→T2→T3）', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## セクションC: AOIレベル分析（注視配分）\n",
    "\n",
    "補正済み固視データと座標定義から、本文/選択肢/問題文への注視配分比率を算出し群間比較する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- C-1: AOI注視配分の算出 ---\n",
    "# 補正パラメータの読み込み\n",
    "corrections_path = WORKING_ROOT / 'corrections' / 'all_segment_corrections.csv'\n",
    "all_corrections = pd.read_csv(corrections_path)\n",
    "print(f'補正パラメータ: {len(all_corrections)}行')\n",
    "\n",
    "aoi_results = []\n",
    "aoi_errors = []\n",
    "aoi_processed = 0\n",
    "\n",
    "for group, pids in participants.items():\n",
    "    for pid in pids:\n",
    "        for phase in ['pre', 'post']:\n",
    "            label = f'{group}/{pid}/{phase}'\n",
    "            try:\n",
    "                base_dir = INPUT_ROOT / group / pid / phase\n",
    "                eye_tracking_base = base_dir / 'eye_tracking'\n",
    "                log_dir = base_dir / 'logs'\n",
    "                coord_dir = INPUT_ROOT / group / 'Test' / phase / 'coordinates'\n",
    "\n",
    "                if not all(d.is_dir() for d in [eye_tracking_base, log_dir, coord_dir]):\n",
    "                    continue\n",
    "\n",
    "                ts_dirs = sorted([d for d in os.listdir(eye_tracking_base)\n",
    "                                  if os.path.isdir(os.path.join(eye_tracking_base, d))])\n",
    "                if not ts_dirs:\n",
    "                    continue\n",
    "                eye_dir = str(eye_tracking_base / ts_dirs[-1])\n",
    "\n",
    "                evt_files = sorted([f for f in os.listdir(log_dir) if f.endswith('.jsonl')])\n",
    "                if not evt_files:\n",
    "                    continue\n",
    "                evt_path = str(log_dir / evt_files[-1])\n",
    "\n",
    "                segments = eg.readTobiiData(eye_dir, evt_path, phase=phase)\n",
    "                coord_mapping = eg.buildCoordinateMapping(str(coord_dir))\n",
    "\n",
    "                p_corr = all_corrections[\n",
    "                    (all_corrections['participant'] == pid) & (all_corrections['phase'] == phase)\n",
    "                ].copy()\n",
    "\n",
    "                for seg_idx, seg in enumerate(segments):\n",
    "                    evt = seg.get('event_type', '')\n",
    "                    if evt != 'question_screen_open':\n",
    "                        continue\n",
    "\n",
    "                    data = seg.get('data')\n",
    "                    if data is None or len(data) == 0:\n",
    "                        continue\n",
    "\n",
    "                    seg_id = seg.get('passage_id')\n",
    "                    image_path = seg.get('image_path', '')\n",
    "                    prefix = eg._eventTypeToCoordPrefix(evt)\n",
    "                    coord_path = coord_mapping.get((prefix, seg_id))\n",
    "                    if not coord_path:\n",
    "                        continue\n",
    "\n",
    "                    fixations = eg.detectFixations(\n",
    "                        data[:, 0], data[:, 1], data[:, 2], P=data[:, 3],\n",
    "                        min_concat_gaze_count=9, min_fixation_size=20, max_fixation_size=40)\n",
    "                    if len(fixations) == 0:\n",
    "                        continue\n",
    "\n",
    "                    # 補正適用\n",
    "                    crow = p_corr[p_corr['segment_index'] == seg_idx]\n",
    "                    if len(crow) > 0:\n",
    "                        r = crow.iloc[0]\n",
    "                        fixations = eg.applyScalingAndOffset(\n",
    "                            fixations, scale_x=r['scale_x'], scale_y=r['scale_y'],\n",
    "                            offset_x=r['offset_x'], offset_y=r['offset_y'])\n",
    "\n",
    "                    coordinates = eg.loadCoordinates(coord_path)\n",
    "                    aoi_params = {}\n",
    "                    if image_path:\n",
    "                        parsed = eg.parseImageFilename(image_path)\n",
    "                        if parsed:\n",
    "                            aoi_params = {\n",
    "                                'target_locale': parsed['target_locale'],\n",
    "                                'target_question': parsed['target_question'],\n",
    "                                'target_analog': parsed['target_analog'],\n",
    "                            }\n",
    "\n",
    "                    # 全レベルAOI抽出\n",
    "                    aois_all = eg.extractAllAOIs(coordinates,\n",
    "                        levels=['sentence', 'title', 'subtitle', 'table', 'choice', 'question', 'metadata', 'instruction'],\n",
    "                        **aoi_params)\n",
    "                    if len(aois_all) == 0:\n",
    "                        continue\n",
    "\n",
    "                    seg_start = fixations[0, 0]\n",
    "                    per_aoi = eg.computePerAOIStatistics(fixations, aois_all,\n",
    "                                                         segment_start=seg_start, tolerance=TOLERANCE)\n",
    "\n",
    "                    # カテゴリ別に注視時間を集計\n",
    "                    passage_levels = {'sentence', 'metadata', 'title', 'subtitle', 'table', 'instruction'}\n",
    "                    choice_levels = {'choice'}\n",
    "                    question_levels = {'question'}\n",
    "\n",
    "                    dur_passage = sum(s['total_duration'] for s in per_aoi if s['level'] in passage_levels)\n",
    "                    dur_choice = sum(s['total_duration'] for s in per_aoi if s['level'] in choice_levels)\n",
    "                    dur_question = sum(s['total_duration'] for s in per_aoi if s['level'] in question_levels)\n",
    "                    dur_total = dur_passage + dur_choice + dur_question\n",
    "\n",
    "                    # FFT（文レベル）\n",
    "                    sent_stats = [s for s in per_aoi if s['level'] == 'sentence']\n",
    "                    fft_values = [s['first_fixation_time'] for s in sent_stats if s['first_fixation_time'] is not None]\n",
    "                    mean_fft = np.mean(fft_values) if fft_values else None\n",
    "\n",
    "                    # 再訪問: AOI fixation_count > 1 の割合\n",
    "                    revisit_aois = sum(1 for s in sent_stats if s['fixation_count'] > 1)\n",
    "                    total_visited = sum(1 for s in sent_stats if s['fixation_count'] > 0)\n",
    "                    revisit_rate = revisit_aois / total_visited if total_visited > 0 else 0\n",
    "\n",
    "                    aoi_results.append({\n",
    "                        'group': group, 'participant': pid, 'phase': phase,\n",
    "                        'passage_id': seg_id, 'segment_index': seg_idx,\n",
    "                        'dur_passage': dur_passage, 'dur_choice': dur_choice,\n",
    "                        'dur_question': dur_question, 'dur_total': dur_total,\n",
    "                        'passage_ratio': dur_passage / dur_total if dur_total > 0 else 0,\n",
    "                        'choice_ratio': dur_choice / dur_total if dur_total > 0 else 0,\n",
    "                        'question_ratio': dur_question / dur_total if dur_total > 0 else 0,\n",
    "                        'mean_sent_fft': mean_fft,\n",
    "                        'revisit_rate': revisit_rate,\n",
    "                    })\n",
    "\n",
    "                aoi_processed += 1\n",
    "            except Exception as e:\n",
    "                import traceback\n",
    "                aoi_errors.append((label, traceback.format_exc()))\n",
    "\n",
    "print(f'AOI処理完了: {aoi_processed} 参加者×フェーズ')\n",
    "print(f'結果レコード数: {len(aoi_results)}')\n",
    "if aoi_errors:\n",
    "    print(f'エラー: {len(aoi_errors)}件')\n",
    "    for lbl, msg in aoi_errors[:3]:\n",
    "        print(f'  {lbl}: {msg.splitlines()[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- C-2: AOI DataFrame & 参加者集約 ---\n",
    "df_aoi = pd.DataFrame(aoi_results)\n",
    "print(f'AOI DataFrame: {df_aoi.shape}')\n",
    "\n",
    "aoi_metrics = ['passage_ratio', 'choice_ratio', 'question_ratio', 'mean_sent_fft', 'revisit_rate']\n",
    "aoi_metric_labels = {\n",
    "    'passage_ratio': '本文注視比率',\n",
    "    'choice_ratio': '選択肢注視比率',\n",
    "    'question_ratio': '問題文注視比率',\n",
    "    'mean_sent_fft': '文レベル平均FFT (秒)',\n",
    "    'revisit_rate': '再訪問率',\n",
    "}\n",
    "\n",
    "df_aoi_p = df_aoi.groupby(['group', 'participant', 'phase'])[aoi_metrics].mean().reset_index()\n",
    "print(f'参加者レベル AOI DF: {df_aoi_p.shape}')\n",
    "df_aoi_p.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- C-3: AOI 記述統計 ---\n",
    "print('【AOI注視配分 記述統計】')\n",
    "print('=' * 70)\n",
    "\n",
    "for metric in aoi_metrics:\n",
    "    print(f'\\n--- {aoi_metric_labels[metric]} ---')\n",
    "    summary = df_aoi_p.groupby(['group', 'phase'])[metric].agg(['mean', 'std', 'count'])\n",
    "    summary.columns = ['M', 'SD', 'n']\n",
    "    summary['M(SD)'] = summary.apply(lambda r: f\"{r['M']:.4f} ({r['SD']:.4f})\", axis=1)\n",
    "    pivot = summary['M(SD)'].unstack(level='phase')\n",
    "    phase_order = [p for p in ['pre', 'post'] if p in pivot.columns]\n",
    "    print(pivot[phase_order])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- C-3b: 正規性検定 (Shapiro-Wilk) — AOI指標 ---\n",
    "print('【正規性検定 (Shapiro-Wilk) — AOI注視配分指標】')\n",
    "print('=' * 70)\n",
    "print('※ p > 0.05 で正規性を仮定（○）、p ≤ 0.05 で棄却（×）')\n",
    "\n",
    "aoi_norm_rows = []\n",
    "\n",
    "for metric in aoi_metrics:\n",
    "    for group in ['A', 'B']:\n",
    "        pre_v, post_v = get_paired_data(df_aoi_p, group, metric)\n",
    "        if len(pre_v) < 3:\n",
    "            continue\n",
    "        diff = post_v - pre_v\n",
    "\n",
    "        w_pre, p_pre = stats.shapiro(pre_v)\n",
    "        w_post, p_post = stats.shapiro(post_v)\n",
    "        w_diff, p_diff = stats.shapiro(diff)\n",
    "\n",
    "        aoi_norm_rows.append({\n",
    "            '指標': aoi_metric_labels[metric],\n",
    "            '群': group,\n",
    "            'n': len(pre_v),\n",
    "            'Pre W': f'{w_pre:.3f}',\n",
    "            'Pre p': f'{p_pre:.3f}',\n",
    "            'Pre 正規性': '○' if p_pre > 0.05 else '×',\n",
    "            'Post W': f'{w_post:.3f}',\n",
    "            'Post p': f'{p_post:.3f}',\n",
    "            'Post 正規性': '○' if p_post > 0.05 else '×',\n",
    "            '差分 W': f'{w_diff:.3f}',\n",
    "            '差分 p': f'{p_diff:.3f}',\n",
    "            '差分 正規性': '○' if p_diff > 0.05 else '×',\n",
    "        })\n",
    "\n",
    "norm_aoi_df = pd.DataFrame(aoi_norm_rows)\n",
    "print()\n",
    "display(norm_aoi_df)\n",
    "\n",
    "# 群間比較用: ゲインスコアの正規性\n",
    "print('\\n【AOIゲインスコアの正規性検定（群間比較の前提確認）】')\n",
    "print('-' * 70)\n",
    "\n",
    "aoi_gain_norm_rows = []\n",
    "for metric in aoi_metrics:\n",
    "    pre_A, post_A = get_paired_data(df_aoi_p, 'A', metric)\n",
    "    pre_B, post_B = get_paired_data(df_aoi_p, 'B', metric)\n",
    "    if len(pre_A) < 3 or len(pre_B) < 3:\n",
    "        continue\n",
    "    gain_A = post_A - pre_A\n",
    "    gain_B = post_B - pre_B\n",
    "\n",
    "    w_A, p_A = stats.shapiro(gain_A)\n",
    "    w_B, p_B = stats.shapiro(gain_B)\n",
    "\n",
    "    aoi_gain_norm_rows.append({\n",
    "        '指標': aoi_metric_labels[metric],\n",
    "        'A群 W': f'{w_A:.3f}',\n",
    "        'A群 p': f'{p_A:.3f}',\n",
    "        'A群 正規性': '○' if p_A > 0.05 else '×',\n",
    "        'B群 W': f'{w_B:.3f}',\n",
    "        'B群 p': f'{p_B:.3f}',\n",
    "        'B群 正規性': '○' if p_B > 0.05 else '×',\n",
    "    })\n",
    "\n",
    "aoi_gain_norm_df = pd.DataFrame(aoi_gain_norm_rows)\n",
    "display(aoi_gain_norm_df)\n",
    "\n",
    "# 判定サマリー\n",
    "aoi_non_normal = norm_aoi_df[norm_aoi_df['差分 正規性'] == '×']\n",
    "if len(aoi_non_normal) > 0:\n",
    "    print(f'\\n⚠ 差分が非正規: {len(aoi_non_normal)}件 → 対応ありt検定の代わりにWilcoxon推奨')\n",
    "    for _, row in aoi_non_normal.iterrows():\n",
    "        print(f\"  - {row['指標']} ({row['群']}群)\")\n",
    "else:\n",
    "    print('\\n✓ 全AOI指標・全群で差分の正規性が確認された → 対応ありt検定が適用可能')\n",
    "\n",
    "aoi_gain_non_normal = aoi_gain_norm_df[\n",
    "    (aoi_gain_norm_df['A群 正規性'] == '×') | (aoi_gain_norm_df['B群 正規性'] == '×')\n",
    "]\n",
    "if len(aoi_gain_non_normal) > 0:\n",
    "    print(f'⚠ ゲインが非正規: {len(aoi_gain_non_normal)}件 → 独立t検定の代わりにMann-Whitney U推奨')\n",
    "    for _, row in aoi_gain_non_normal.iterrows():\n",
    "        flags = []\n",
    "        if row['A群 正規性'] == '×': flags.append('A群')\n",
    "        if row['B群 正規性'] == '×': flags.append('B群')\n",
    "        print(f\"  - {row['指標']} ({', '.join(flags)})\")\n",
    "else:\n",
    "    print('✓ 全AOI指標でゲインスコアの正規性が確認された → 独立t検定が適用可能')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- C-4: AOI 群内比較 + 群間比較 + ANOVA ---\n",
    "print('【AOI Pre-Post比較】')\n",
    "print('=' * 70)\n",
    "\n",
    "aoi_within = []\n",
    "aoi_between = []\n",
    "aoi_anova = []\n",
    "\n",
    "for metric in aoi_metrics:\n",
    "    print(f'\\n--- {aoi_metric_labels[metric]} ---')\n",
    "\n",
    "    # 群内\n",
    "    for group in ['A', 'B']:\n",
    "        pre_v, post_v = get_paired_data(df_aoi_p, group, metric)\n",
    "        if len(pre_v) < 3:\n",
    "            continue\n",
    "        diff = post_v - pre_v\n",
    "        t_stat, t_p = stats.ttest_rel(post_v, pre_v)\n",
    "        d = cohens_d_paired(pre_v, post_v)\n",
    "        print(f'  {group}群 (n={len(pre_v)}): Pre={np.mean(pre_v):.4f} -> Post={np.mean(post_v):.4f} '\n",
    "              f'差={np.mean(diff):+.4f}, t={t_stat:.3f}, p={t_p:.3f}, d={d:.3f}')\n",
    "        aoi_within.append({\n",
    "            '指標': aoi_metric_labels[metric], '群': group, 'n': len(pre_v),\n",
    "            'Pre平均': np.mean(pre_v), 'Post平均': np.mean(post_v),\n",
    "            '差': np.mean(diff), 't': t_stat, 'p': t_p, 'd': d,\n",
    "        })\n",
    "\n",
    "    # 群間ゲイン\n",
    "    pre_A, post_A = get_paired_data(df_aoi_p, 'A', metric)\n",
    "    pre_B, post_B = get_paired_data(df_aoi_p, 'B', metric)\n",
    "    if len(pre_A) >= 3 and len(pre_B) >= 3:\n",
    "        gain_A = post_A - pre_A\n",
    "        gain_B = post_B - pre_B\n",
    "        t_stat, t_p = stats.ttest_ind(gain_A, gain_B)\n",
    "        u_stat, u_p = stats.mannwhitneyu(gain_A, gain_B, alternative='two-sided')\n",
    "        d = cohens_d_ind(gain_B, gain_A)\n",
    "        ci_lo, ci_hi = bootstrap_ci(gain_B, gain_A, cohens_d_ind)\n",
    "        print(f'  ゲイン A: {np.mean(gain_A):+.4f}, B: {np.mean(gain_B):+.4f} '\n",
    "              f'| t={t_stat:.3f}, p={t_p:.3f}, d={d:.3f} [{ci_lo:.3f},{ci_hi:.3f}]')\n",
    "        aoi_between.append({\n",
    "            '指標': aoi_metric_labels[metric],\n",
    "            'A群ゲイン': np.mean(gain_A), 'B群ゲイン': np.mean(gain_B),\n",
    "            't': t_stat, 'p_t': t_p, 'U': u_stat, 'p_MW': u_p,\n",
    "            'd': d, 'CI下限': ci_lo, 'CI上限': ci_hi,\n",
    "        })\n",
    "\n",
    "        # ANOVA\n",
    "        res = mixed_anova(pre_A, post_A, pre_B, post_B)\n",
    "        print(f'  ANOVA 交互作用: F={res[\"F_interaction\"]:.3f}, p={res[\"p_interaction\"]:.3f}, '\n",
    "              f'η²p={res[\"eta2_interaction\"]:.3f}')\n",
    "        res['指標'] = aoi_metric_labels[metric]\n",
    "        aoi_anova.append(res)\n",
    "\n",
    "aoi_within_df = pd.DataFrame(aoi_within)\n",
    "aoi_between_df = pd.DataFrame(aoi_between)\n",
    "aoi_anova_df = pd.DataFrame(aoi_anova)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- C-5: AOI 可視化 ---\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, metric in enumerate(aoi_metrics):\n",
    "    ax = axes[idx]\n",
    "    means, sems, colors = [], [], []\n",
    "    for group, color in [('A', '#4C72B0'), ('B', '#DD8452')]:\n",
    "        for ph in ['pre', 'post']:\n",
    "            vals = df_aoi_p[(df_aoi_p['group'] == group) & (df_aoi_p['phase'] == ph)][metric].dropna().values\n",
    "            means.append(np.mean(vals) if len(vals) > 0 else 0)\n",
    "            sems.append(np.std(vals, ddof=1) / np.sqrt(len(vals)) if len(vals) > 1 else 0)\n",
    "            colors.append(color)\n",
    "    x = np.arange(4)\n",
    "    ax.bar(x, means, yerr=sems, capsize=4, color=colors, alpha=0.8, edgecolor='black', linewidth=0.5)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(['A\\npre', 'A\\npost', 'B\\npre', 'B\\npost'])\n",
    "    ax.set_title(aoi_metric_labels[metric])\n",
    "\n",
    "for i in range(len(aoi_metrics), len(axes)):\n",
    "    axes[i].set_visible(False)\n",
    "plt.suptitle('AOI注視配分: Pre vs Post（群別）', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- C-6: AOI ゲインスコア ボックスプロット ---\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, metric in enumerate(aoi_metrics):\n",
    "    ax = axes[idx]\n",
    "    gain_data, labels = [], []\n",
    "    for group in ['A', 'B']:\n",
    "        pre_v, post_v = get_paired_data(df_aoi_p, group, metric)\n",
    "        gains = post_v - pre_v\n",
    "        gain_data.append(gains)\n",
    "        labels.append(f'{group}群\\n(n={len(gains)})')\n",
    "    bp = ax.boxplot(gain_data, tick_labels=labels, patch_artist=True,\n",
    "                    medianprops=dict(color='black', linewidth=2))\n",
    "    bp['boxes'][0].set_facecolor('#4C72B0')\n",
    "    bp['boxes'][0].set_alpha(0.7)\n",
    "    bp['boxes'][1].set_facecolor('#DD8452')\n",
    "    bp['boxes'][1].set_alpha(0.7)\n",
    "    ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "    ax.set_title(f'{aoi_metric_labels[metric]}\\n(Post - Pre)')\n",
    "    ax.set_ylabel('ゲイン')\n",
    "\n",
    "for i in range(len(aoi_metrics), len(axes)):\n",
    "    axes[i].set_visible(False)\n",
    "plt.suptitle('AOI: ゲインスコア (Post - Pre)', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- C-7: 選択肢注視比率 個人別ゲインスコア ---\n",
    "metric = 'choice_ratio'\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "for i, group in enumerate(['A', 'B']):\n",
    "    sub = df_aoi_p[df_aoi_p['group'] == group]\n",
    "    pre = sub[sub['phase'] == 'pre'].set_index('participant')[metric]\n",
    "    post = sub[sub['phase'] == 'post'].set_index('participant')[metric]\n",
    "    common = pre.index.intersection(post.index)\n",
    "    pre_v, post_v = pre.loc[common], post.loc[common]\n",
    "    gains = post_v - pre_v\n",
    "\n",
    "    x = np.full(len(gains), i)\n",
    "    jitter = np.random.default_rng(42).uniform(-0.08, 0.08, size=len(gains))\n",
    "    color = '#4C72B0' if group == 'A' else '#DD8452'\n",
    "    ax.scatter(x + jitter, gains.values, color=color, s=80, alpha=0.7, zorder=3, edgecolors='white')\n",
    "    for xi, ji, yi, pid in zip(x, jitter, gains.values, gains.index):\n",
    "        ax.annotate(pid, (xi + ji, yi), fontsize=8, ha='left', va='bottom',\n",
    "                    xytext=(5, 2), textcoords='offset points')\n",
    "\n",
    "    m, sd = gains.mean(), gains.std()\n",
    "    ax.hlines(m, i - 0.2, i + 0.2, colors=color, linewidths=3, zorder=4, label=f'{group}群 M={m:.4f}')\n",
    "    print(f'{group}群: M={m:.4f}, SD={sd:.4f}, n={len(gains)}')\n",
    "    for pid, val in gains.items():\n",
    "        print(f'  {pid}: {val:+.4f}')\n",
    "\n",
    "ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.set_xticks([0, 1])\n",
    "ax.set_xticklabels(['A群', 'B群'])\n",
    "ax.set_ylabel('選択肢注視比率ゲイン (Post - Pre)')\n",
    "ax.set_title('選択肢注視比率: 個人別ゲインスコア')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## セクションD: 探索的分析\n",
    "\n",
    "サッカード分析、瞳孔径分析、時間経過分析（前半/後半の注視配分変化）、ベースライン交互作用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- D-3: 瞳孔径分析 ---\n",
    "print('【瞳孔径（認知負荷指標）分析】')\n",
    "print('=' * 70)\n",
    "\n",
    "metric = 'mean_pupil_diameter'\n",
    "print(f'\\n--- {seg_metric_labels[metric]} ---')\n",
    "\n",
    "# 群内\n",
    "for group in ['A', 'B']:\n",
    "    pre_v, post_v = get_paired_data(df_seg, group, metric)\n",
    "    if len(pre_v) < 3:\n",
    "        continue\n",
    "    diff = post_v - pre_v\n",
    "    t_stat, t_p = stats.ttest_rel(post_v, pre_v)\n",
    "    d = cohens_d_paired(pre_v, post_v)\n",
    "    print(f'  {group}群: Pre={np.mean(pre_v):.3f} -> Post={np.mean(post_v):.3f}, '\n",
    "          f'差={np.mean(diff):+.3f}, t={t_stat:.3f}, p={t_p:.3f}, d={d:.3f}')\n",
    "\n",
    "# 群間\n",
    "pre_A, post_A = get_paired_data(df_seg, 'A', metric)\n",
    "pre_B, post_B = get_paired_data(df_seg, 'B', metric)\n",
    "if len(pre_A) >= 3 and len(pre_B) >= 3:\n",
    "    gain_A = post_A - pre_A\n",
    "    gain_B = post_B - pre_B\n",
    "    t_stat, t_p = stats.ttest_ind(gain_A, gain_B)\n",
    "    d = cohens_d_ind(gain_B, gain_A)\n",
    "    print(f'\\n  群間ゲイン: A={np.mean(gain_A):+.3f}, B={np.mean(gain_B):+.3f}')\n",
    "    print(f'  t={t_stat:.3f}, p={t_p:.3f}, d={d:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- D-4: 時間経過分析（前半/後半の注視配分変化） ---\n",
    "# 各問題画面を前半・後半に分割し、注視配分の変化を見る\n",
    "half_results = []\n",
    "\n",
    "for _, row in df_aoi.iterrows():\n",
    "    group, pid, phase = row['group'], row['participant'], row['phase']\n",
    "    seg_idx = int(row['segment_index'])\n",
    "    img_num = None\n",
    "\n",
    "    # 対応する統計CSVから画像番号を取得\n",
    "    st_path = OUTPUT_ROOT / group / pid / phase / 'statistics.csv'\n",
    "    if st_path.exists():\n",
    "        st = pd.read_csv(st_path)\n",
    "        q_rows = st[st['event_type'] == 'question_screen_open']\n",
    "        # seg_indexに対応する行を探す\n",
    "        for i, (_, sr) in enumerate(q_rows.iterrows()):\n",
    "            if i == (seg_idx - 1):  # seg_idx は1-indexed (phase_intro=0)\n",
    "                img_num = str(int(sr['image_number'])).zfill(3)\n",
    "                break\n",
    "\n",
    "    if img_num is None:\n",
    "        continue\n",
    "\n",
    "    fix_path = WORKING_ROOT / group / pid / phase / 'fixation_corrected' / f'{img_num}.csv'\n",
    "    if not fix_path.exists():\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        fix_df = pd.read_csv(fix_path)\n",
    "        if len(fix_df) < 4:\n",
    "            continue\n",
    "        mid = len(fix_df) // 2\n",
    "        # 前半/後半での平均x座標（左=本文領域, 右=選択肢領域, 閾値~480px）\n",
    "        x_first = fix_df['x'].iloc[:mid].mean()\n",
    "        x_second = fix_df['x'].iloc[mid:].mean()\n",
    "        # 本文領域は x < 480 と仮定（左パネル）\n",
    "        passage_fix_first = (fix_df['x'].iloc[:mid] < 480).mean()\n",
    "        passage_fix_second = (fix_df['x'].iloc[mid:] < 480).mean()\n",
    "\n",
    "        half_results.append({\n",
    "            'group': group, 'participant': pid, 'phase': phase,\n",
    "            '前半本文比率': passage_fix_first,\n",
    "            '後半本文比率': passage_fix_second,\n",
    "            'シフト': passage_fix_second - passage_fix_first,\n",
    "        })\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "df_half = pd.DataFrame(half_results)\n",
    "if len(df_half) > 0:\n",
    "    df_half_p = df_half.groupby(['group', 'participant', 'phase'])[\n",
    "        ['前半本文比率', '後半本文比率', 'シフト']].mean().reset_index()\n",
    "\n",
    "    print('【前半/後半の本文注視比率】')\n",
    "    print('=' * 70)\n",
    "    for group in ['A', 'B']:\n",
    "        for ph in ['pre', 'post']:\n",
    "            vals = df_half_p[(df_half_p['group'] == group) & (df_half_p['phase'] == ph)]\n",
    "            if len(vals) > 0:\n",
    "                print(f'  {group}/{ph}: 前半={vals[\"前半本文比率\"].mean():.3f}, '\n",
    "                      f'後半={vals[\"後半本文比率\"].mean():.3f}, '\n",
    "                      f'シフト={vals[\"シフト\"].mean():+.3f}')\n",
    "\n",
    "    # シフトの群間比較\n",
    "    pre_A, post_A = get_paired_data(df_half_p, 'A', 'シフト')\n",
    "    pre_B, post_B = get_paired_data(df_half_p, 'B', 'シフト')\n",
    "    if len(pre_A) >= 3 and len(pre_B) >= 3:\n",
    "        gain_A = post_A - pre_A\n",
    "        gain_B = post_B - pre_B\n",
    "        t_stat, t_p = stats.ttest_ind(gain_A, gain_B)\n",
    "        print(f'\\n  シフト ゲイン比較: t={t_stat:.3f}, p={t_p:.3f}')\n",
    "else:\n",
    "    print('前半/後半データなし')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- D-5: ベースライン交互作用（Pre得点による調整） ---\n",
    "# score-summaryからpre得点を取得\n",
    "print('【ベースライン交互作用分析】')\n",
    "print('=' * 70)\n",
    "\n",
    "pre_scores = {}\n",
    "for group, pids in participants.items():\n",
    "    for pid in pids:\n",
    "        log_dir = INPUT_ROOT / group / pid / 'pre' / 'logs'\n",
    "        if not log_dir.is_dir():\n",
    "            continue\n",
    "        evt_files = sorted([f for f in os.listdir(log_dir) if f.endswith('.jsonl')])\n",
    "        if not evt_files:\n",
    "            continue\n",
    "        correct, total = 0, 0\n",
    "        with open(log_dir / evt_files[-1], 'r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                try:\n",
    "                    event = json.loads(line)\n",
    "                    if event.get('event') == 'answer_submit':\n",
    "                        correct += event.get('correct_count', 0)\n",
    "                        total += event.get('total_count', 0)\n",
    "                except json.JSONDecodeError:\n",
    "                    pass\n",
    "        if total > 0:\n",
    "            pre_scores[pid] = correct / total * 100\n",
    "\n",
    "print(f'Pre得点取得: {len(pre_scores)}名')\n",
    "\n",
    "# 主要指標のゲインとPre得点の関係\n",
    "key_metrics_for_baseline = [\n",
    "    ('mean_fixation_duration', df_seg, seg_metric_labels),\n",
    "]\n",
    "if len(df_aoi_p) > 0:\n",
    "    key_metrics_for_baseline.append(('passage_ratio', df_aoi_p, aoi_metric_labels))\n",
    "\n",
    "for metric, df_src, label_dict in key_metrics_for_baseline:\n",
    "    print(f'\\n--- {label_dict[metric]} ---')\n",
    "    gains_all = []\n",
    "    pre_scores_all = []\n",
    "    groups_all = []\n",
    "\n",
    "    for group in ['A', 'B']:\n",
    "        pre_v_df = df_src[(df_src['group'] == group) & (df_src['phase'] == 'pre')].set_index('participant')[metric]\n",
    "        post_v_df = df_src[(df_src['group'] == group) & (df_src['phase'] == 'post')].set_index('participant')[metric]\n",
    "        common = pre_v_df.index.intersection(post_v_df.index)\n",
    "        for pid in common:\n",
    "            if pid in pre_scores and not np.isnan(pre_v_df[pid]) and not np.isnan(post_v_df[pid]):\n",
    "                gains_all.append(post_v_df[pid] - pre_v_df[pid])\n",
    "                pre_scores_all.append(pre_scores[pid])\n",
    "                groups_all.append(group)\n",
    "\n",
    "    gains_arr = np.array(gains_all)\n",
    "    scores_arr = np.array(pre_scores_all)\n",
    "    groups_arr = np.array(groups_all)\n",
    "\n",
    "    if len(gains_arr) >= 6:\n",
    "        # 中央値分割: 低スコア vs 高スコア\n",
    "        median_score = np.median(scores_arr)\n",
    "        print(f'  Pre得点中央値: {median_score:.1f}%')\n",
    "\n",
    "        for score_label, mask in [('低得点', scores_arr <= median_score),\n",
    "                                   ('高得点', scores_arr > median_score)]:\n",
    "            for g in ['A', 'B']:\n",
    "                g_mask = mask & (groups_arr == g)\n",
    "                vals = gains_arr[g_mask]\n",
    "                if len(vals) > 0:\n",
    "                    print(f'    {score_label} {g}群: n={len(vals)}, gain M={np.mean(vals):+.4f}')\n",
    "\n",
    "        # 低得点者のみの群間比較\n",
    "        low_A = gains_arr[mask & (groups_arr == 'A')]\n",
    "        low_B = gains_arr[mask & (groups_arr == 'B')]\n",
    "        if len(low_A) >= 2 and len(low_B) >= 2:\n",
    "            t_stat, t_p = stats.ttest_ind(low_A, low_B)\n",
    "            print(f'  低得点者 群間: t={t_stat:.3f}, p={t_p:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## セクションE: 統合的統計まとめ\n",
    "\n",
    "多重比較補正、ベイズt検定、検出力分析、効果量サマリーテーブル。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- E-1: 全比較結果の統合 ---\n",
    "all_comparisons = []\n",
    "\n",
    "# セグメントレベル\n",
    "for _, row in between_df.iterrows():\n",
    "    all_comparisons.append({\n",
    "        'ファミリー': 'セグメント', '指標': row['指標'],\n",
    "        'A群ゲイン': row['A群ゲイン平均'], 'B群ゲイン': row['B群ゲイン平均'],\n",
    "        'p': row['p_t'], 'd': row['Cohen_d'],\n",
    "        'CI下限': row['CI下限'], 'CI上限': row['CI上限'],\n",
    "    })\n",
    "\n",
    "# AOIレベル\n",
    "if len(aoi_between_df) > 0:\n",
    "    for _, row in aoi_between_df.iterrows():\n",
    "        all_comparisons.append({\n",
    "            'ファミリー': 'AOI', '指標': row['指標'],\n",
    "            'A群ゲイン': row['A群ゲイン'], 'B群ゲイン': row['B群ゲイン'],\n",
    "            'p': row['p_t'], 'd': row['d'],\n",
    "            'CI下限': row['CI下限'], 'CI上限': row['CI上限'],\n",
    "        })\n",
    "\n",
    "df_comp = pd.DataFrame(all_comparisons)\n",
    "print(f'全比較数: {len(df_comp)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- E-2: Benjamini-Hochberg FDR補正 ---\n",
    "print('【多重比較補正 (Benjamini-Hochberg FDR)】')\n",
    "print('=' * 70)\n",
    "\n",
    "for family in df_comp['ファミリー'].unique():\n",
    "    mask = df_comp['ファミリー'] == family\n",
    "    p_vals = df_comp.loc[mask, 'p'].values\n",
    "    m = len(p_vals)\n",
    "    if m == 0:\n",
    "        continue\n",
    "    # BH手続き\n",
    "    sorted_idx = np.argsort(p_vals)\n",
    "    p_adj = np.zeros(m)\n",
    "    for rank_i, orig_i in enumerate(sorted_idx):\n",
    "        p_adj[orig_i] = p_vals[orig_i] * m / (rank_i + 1)\n",
    "    # 単調性の強制\n",
    "    for i in range(m - 2, -1, -1):\n",
    "        idx = sorted_idx[i]\n",
    "        idx_next = sorted_idx[i + 1]\n",
    "        p_adj[idx] = min(p_adj[idx], p_adj[idx_next])\n",
    "    p_adj = np.minimum(p_adj, 1.0)\n",
    "    df_comp.loc[mask, 'p_adj'] = p_adj\n",
    "\n",
    "    print(f'\\n{family} ファミリー ({m}比較):')\n",
    "    sub = df_comp[mask][['指標', 'p', 'p_adj', 'd']].copy()\n",
    "    sub['判定'] = sub['p_adj'].apply(lambda x: '*' if x < 0.05 else 'n.s.')\n",
    "    print(sub.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- E-3: ベイズt検定 ---\n",
    "print('【ベイズt検定 (JZS事前分布)】')\n",
    "print('=' * 70)\n",
    "\n",
    "def bayes_t_test_ind(x, y, r=0.707):\n",
    "    \"\"\"JZS事前分布によるベイズt検定（独立サンプル）\"\"\"\n",
    "    from scipy.stats import t as t_dist\n",
    "    nx, ny = len(x), len(y)\n",
    "    n = nx + ny\n",
    "    pooled_var = ((nx-1)*np.var(x, ddof=1) + (ny-1)*np.var(y, ddof=1)) / (n-2)\n",
    "    se = np.sqrt(pooled_var * (1/nx + 1/ny))\n",
    "    t_val = (np.mean(x) - np.mean(y)) / se if se > 0 else 0\n",
    "    df = n - 2\n",
    "    neff = (nx * ny) / (nx + ny)\n",
    "\n",
    "    lik_h0 = t_dist.pdf(t_val, df)\n",
    "\n",
    "    def integrand(delta):\n",
    "        return t_dist.pdf(t_val, df, loc=delta * np.sqrt(neff)) * stats.cauchy.pdf(delta, scale=r)\n",
    "\n",
    "    marg_h1, _ = quad(integrand, -np.inf, np.inf)\n",
    "    bf10 = marg_h1 / lik_h0 if lik_h0 > 0 else np.inf\n",
    "    return bf10, 1/bf10 if bf10 > 0 else np.inf\n",
    "\n",
    "\n",
    "bf_results = []\n",
    "\n",
    "# セグメントレベル\n",
    "for metric in seg_metrics:\n",
    "    pre_A, post_A = get_paired_data(df_seg, 'A', metric)\n",
    "    pre_B, post_B = get_paired_data(df_seg, 'B', metric)\n",
    "    if len(pre_A) < 3 or len(pre_B) < 3:\n",
    "        continue\n",
    "    gain_A = post_A - pre_A\n",
    "    gain_B = post_B - pre_B\n",
    "    bf10, bf01 = bayes_t_test_ind(gain_A, gain_B)\n",
    "    bf_results.append({'ファミリー': 'セグメント', '指標': seg_metric_labels[metric], 'BF10': bf10, 'BF01': bf01})\n",
    "    print(f'  セグメント/{seg_metric_labels[metric]}: BF10={bf10:.3f}, BF01={bf01:.3f}')\n",
    "\n",
    "# AOIレベル\n",
    "for metric in aoi_metrics:\n",
    "    pre_A, post_A = get_paired_data(df_aoi_p, 'A', metric)\n",
    "    pre_B, post_B = get_paired_data(df_aoi_p, 'B', metric)\n",
    "    if len(pre_A) < 3 or len(pre_B) < 3:\n",
    "        continue\n",
    "    gain_A = post_A - pre_A\n",
    "    gain_B = post_B - pre_B\n",
    "    bf10, bf01 = bayes_t_test_ind(gain_A, gain_B)\n",
    "    bf_results.append({'ファミリー': 'AOI', '指標': aoi_metric_labels[metric], 'BF10': bf10, 'BF01': bf01})\n",
    "    print(f'  AOI/{aoi_metric_labels[metric]}: BF10={bf10:.3f}, BF01={bf01:.3f}')\n",
    "\n",
    "bf_df = pd.DataFrame(bf_results)\n",
    "\n",
    "# BF解釈\n",
    "def interpret_bf(bf10):\n",
    "    if bf10 > 10: return 'H1強い証拠'\n",
    "    if bf10 > 3: return 'H1中程度の証拠'\n",
    "    if bf10 > 1: return 'H1弱い証拠'\n",
    "    if 1/bf10 > 10: return 'H0強い証拠'\n",
    "    if 1/bf10 > 3: return 'H0中程度の証拠'\n",
    "    return '判断不能'\n",
    "\n",
    "bf_df['解釈'] = bf_df['BF10'].apply(interpret_bf)\n",
    "print('\\n')\n",
    "bf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- E-4: 検出力分析 ---\n",
    "print('【検出力分析】')\n",
    "print('=' * 70)\n",
    "\n",
    "def power_ind(n1, n2, d, alpha=0.05):\n",
    "    df = n1 + n2 - 2\n",
    "    ncp = d * np.sqrt((n1 * n2) / (n1 + n2))\n",
    "    t_crit = stats.t.ppf(1 - alpha/2, df)\n",
    "    return 1 - nct.cdf(t_crit, df, ncp) + nct.cdf(-t_crit, df, ncp)\n",
    "\n",
    "def required_n(d, target_power=0.80, alpha=0.05):\n",
    "    for n in range(5, 1000):\n",
    "        if power_ind(n, n, d, alpha) >= target_power:\n",
    "            return n\n",
    "    return '>1000'\n",
    "\n",
    "power_rows = []\n",
    "\n",
    "for _, row in df_comp.iterrows():\n",
    "    d_obs = abs(row['d'])\n",
    "    pw = power_ind(10, 10, d_obs) if d_obs > 0 else 0\n",
    "    n_req = required_n(d_obs) if d_obs > 0 else '>1000'\n",
    "    power_rows.append({\n",
    "        'ファミリー': row['ファミリー'], '指標': row['指標'],\n",
    "        '観測d': d_obs, '検出力_n10': pw, '80%必要n': n_req,\n",
    "    })\n",
    "\n",
    "power_df = pd.DataFrame(power_rows)\n",
    "print(power_df.to_string(index=False))\n",
    "\n",
    "# 基準効果量での検出力\n",
    "print('\\n\\n標準効果量での検出力 (各群n=10):')\n",
    "for d_val, label in [(0.2, '小'), (0.5, '中'), (0.8, '大')]:\n",
    "    pw = power_ind(10, 10, d_val)\n",
    "    print(f'  d={d_val} ({label}): 検出力={pw:.3f}, 80%に必要なn={required_n(d_val)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- E-5: 効果量サマリーテーブル ---\n",
    "print('【効果量サマリーテーブル】')\n",
    "print('=' * 70)\n",
    "\n",
    "# BFをマージ\n",
    "summary_df = df_comp.copy()\n",
    "bf_map = {row['指標']: row['BF10'] for _, row in bf_df.iterrows()}\n",
    "summary_df['BF10'] = summary_df['指標'].map(bf_map)\n",
    "\n",
    "# 検出力をマージ\n",
    "pw_map = {(row['ファミリー'], row['指標']): row['検出力_n10'] for _, row in power_df.iterrows()}\n",
    "summary_df['検出力'] = summary_df.apply(lambda r: pw_map.get((r['ファミリー'], r['指標']), np.nan), axis=1)\n",
    "\n",
    "display_cols = ['ファミリー', '指標', 'd', 'p', 'p_adj', 'CI下限', 'CI上限', 'BF10', '検出力']\n",
    "summary_df = summary_df[[c for c in display_cols if c in summary_df.columns]]\n",
    "summary_df = summary_df.round(4)\n",
    "print(summary_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## セクションF: 結論・出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- F-1: 結果まとめ ---\n",
    "print('=' * 70)\n",
    "print('【A群 vs B群 視線行動比較分析 — 結果まとめ】')\n",
    "print('=' * 70)\n",
    "\n",
    "print('\\n■ セクションA: セグメントレベル (Pre-Post ゲインスコア)')\n",
    "for _, row in between_df.iterrows():\n",
    "    sig = '*' if row['p_t'] < 0.05 else 'n.s.'\n",
    "    print(f'  {row[\"指標\"]}: d={row[\"Cohen_d\"]:.3f}, p={row[\"p_t\"]:.3f} {sig}')\n",
    "\n",
    "print('\\n■ セクションA: 混合分散分析 交互作用')\n",
    "for _, row in pd.DataFrame(anova_results).iterrows():\n",
    "    sig = '*' if row['p_interaction'] < 0.05 else 'n.s.'\n",
    "    print(f'  {row[\"指標\"]}: F={row[\"F_interaction\"]:.3f}, p={row[\"p_interaction\"]:.3f}, '\n",
    "          f'η²p={row[\"eta2_interaction\"]:.3f} {sig}')\n",
    "\n",
    "if len(aoi_between_df) > 0:\n",
    "    print('\\n■ セクションC: AOI注視配分 (Pre-Post ゲインスコア)')\n",
    "    for _, row in aoi_between_df.iterrows():\n",
    "        sig = '*' if row['p_t'] < 0.05 else 'n.s.'\n",
    "        print(f'  {row[\"指標\"]}: d={row[\"d\"]:.3f}, p={row[\"p_t\"]:.3f} {sig}')\n",
    "\n",
    "print('\\n■ セクションE: ベイズ分析')\n",
    "for _, row in bf_df.iterrows():\n",
    "    print(f'  {row[\"指標\"]}: BF10={row[\"BF10\"]:.3f} ({row[\"解釈\"]})')\n",
    "\n",
    "print('\\n■ 検出力')\n",
    "for _, row in power_df.iterrows():\n",
    "    print(f'  {row[\"指標\"]}: |d|={row[\"観測d\"]:.3f}, 検出力={row[\"検出力_n10\"]:.3f}, '\n",
    "          f'80%に必要なn={row[\"80%必要n\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- F-2: 論文用サマリーテーブル出力 ---\n",
    "output_path = OUTPUT_ROOT / 'gaze_comparison_results.csv'\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 全結果を統合\n",
    "final_rows = []\n",
    "\n",
    "# セグメントレベル\n",
    "for _, row in between_df.iterrows():\n",
    "    final_rows.append({\n",
    "        '分析': 'セグメント ゲイン', '指標': row['指標'],\n",
    "        'A群平均': row['A群ゲイン平均'], 'A群SD': row['A群ゲインSD'],\n",
    "        'B群平均': row['B群ゲイン平均'], 'B群SD': row['B群ゲインSD'],\n",
    "        't': row['t'], 'p': row['p_t'], 'p_MW': row['p_MW'],\n",
    "        'Cohen_d': row['Cohen_d'], 'CI下限': row['CI下限'], 'CI上限': row['CI上限'],\n",
    "    })\n",
    "\n",
    "# AOI\n",
    "if len(aoi_between_df) > 0:\n",
    "    for _, row in aoi_between_df.iterrows():\n",
    "        final_rows.append({\n",
    "            '分析': 'AOI ゲイン', '指標': row['指標'],\n",
    "            'A群平均': row['A群ゲイン'], 'B群平均': row['B群ゲイン'],\n",
    "            't': row['t'], 'p': row['p_t'], 'p_MW': row['p_MW'],\n",
    "            'Cohen_d': row['d'], 'CI下限': row['CI下限'], 'CI上限': row['CI上限'],\n",
    "        })\n",
    "\n",
    "# ANOVA\n",
    "for _, row in pd.DataFrame(anova_results).iterrows():\n",
    "    final_rows.append({\n",
    "        '分析': '混合分散分析 交互作用', '指標': row['指標'],\n",
    "        'F': row['F_interaction'], 'p': row['p_interaction'],\n",
    "        'η²p': row['eta2_interaction'],\n",
    "    })\n",
    "\n",
    "# BF\n",
    "for _, row in bf_df.iterrows():\n",
    "    final_rows.append({\n",
    "        '分析': 'ベイズファクター', '指標': row['指標'],\n",
    "        'BF10': row['BF10'], 'BF01': row['BF01'],\n",
    "        '解釈': row['解釈'],\n",
    "    })\n",
    "\n",
    "final_df = pd.DataFrame(final_rows)\n",
    "final_df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "print(f'結果保存先: {output_path}')\n",
    "print(f'行数: {len(final_df)}')\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oi5tzzbj3v",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "base_path = Path(\"/Users/kyoya/Laboratory/metacognition-analysis/data/input\")\n",
    "\n",
    "group_a = [\"P001\", \"P002\", \"P005\", \"P006\", \"P008\", \"P009\", \"P010\", \"P011\", \"P016\", \"P017\"]\n",
    "group_b = [\"P003\", \"P004\", \"P007\", \"P012\", \"P013\", \"P014\", \"P015\", \"P018\", \"P019\", \"P020\"]\n",
    "phases = [\"pre\", \"post\", \"training1\", \"training2\", \"training3\"]\n",
    "\n",
    "results = []\n",
    "\n",
    "for group_name, participants in [(\"A\", group_a), (\"B\", group_b)]:\n",
    "    for pid in participants:\n",
    "        for phase in phases:\n",
    "            eye_dir = base_path / group_name / pid / phase / \"eye_tracking\"\n",
    "            \n",
    "            if not eye_dir.exists():\n",
    "                results.append({\n",
    "                    \"Group\": group_name,\n",
    "                    \"Participant\": pid,\n",
    "                    \"Phase\": phase,\n",
    "                    \"Status\": \"MISSING_DIR\",\n",
    "                    \"Total_Duration_s\": np.nan,\n",
    "                    \"Total_Rows\": 0,\n",
    "                    \"Sampling_Rate_Hz\": np.nan,\n",
    "                    \"Valid_Gaze_Pct\": np.nan,\n",
    "                    \"Left_Valid_Pct\": np.nan,\n",
    "                    \"Right_Valid_Pct\": np.nan,\n",
    "                    \"Flag\": \"MISSING\"\n",
    "                })\n",
    "                continue\n",
    "            \n",
    "            # Find the timestamp subdirectory\n",
    "            subdirs = [d for d in eye_dir.iterdir() if d.is_dir()]\n",
    "            if len(subdirs) == 0:\n",
    "                results.append({\n",
    "                    \"Group\": group_name,\n",
    "                    \"Participant\": pid,\n",
    "                    \"Phase\": phase,\n",
    "                    \"Status\": \"NO_SUBDIR\",\n",
    "                    \"Total_Duration_s\": np.nan,\n",
    "                    \"Total_Rows\": 0,\n",
    "                    \"Sampling_Rate_Hz\": np.nan,\n",
    "                    \"Valid_Gaze_Pct\": np.nan,\n",
    "                    \"Left_Valid_Pct\": np.nan,\n",
    "                    \"Right_Valid_Pct\": np.nan,\n",
    "                    \"Flag\": \"MISSING\"\n",
    "                })\n",
    "                continue\n",
    "            \n",
    "            ts_dir = subdirs[0]  # Take first (should be only one)\n",
    "            csv_path = ts_dir / \"tobii_pro_gaze.csv\"\n",
    "            \n",
    "            if not csv_path.exists():\n",
    "                results.append({\n",
    "                    \"Group\": group_name,\n",
    "                    \"Participant\": pid,\n",
    "                    \"Phase\": phase,\n",
    "                    \"Status\": \"NO_CSV\",\n",
    "                    \"Total_Duration_s\": np.nan,\n",
    "                    \"Total_Rows\": 0,\n",
    "                    \"Sampling_Rate_Hz\": np.nan,\n",
    "                    \"Valid_Gaze_Pct\": np.nan,\n",
    "                    \"Left_Valid_Pct\": np.nan,\n",
    "                    \"Right_Valid_Pct\": np.nan,\n",
    "                    \"Flag\": \"MISSING\"\n",
    "                })\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                df = pd.read_csv(csv_path)\n",
    "                \n",
    "                # Columns\n",
    "                ts_col = df.columns[0]  # '#timestamp'\n",
    "                gaze_x_col = 'gaze_x'\n",
    "                gaze_y_col = 'gaze_y'\n",
    "                \n",
    "                total_rows = len(df)\n",
    "                \n",
    "                # Duration in seconds (timestamps are in milliseconds)\n",
    "                timestamps = df[ts_col].values\n",
    "                duration_s = (timestamps[-1] - timestamps[0]) / 1000.0\n",
    "                \n",
    "                # Sampling rate\n",
    "                sampling_rate = total_rows / duration_s if duration_s > 0 else 0\n",
    "                \n",
    "                # Valid gaze: both gaze_x and gaze_y are not NaN\n",
    "                valid_gaze = df[gaze_x_col].notna() & df[gaze_y_col].notna()\n",
    "                valid_gaze_pct = valid_gaze.sum() / total_rows * 100 if total_rows > 0 else 0\n",
    "                \n",
    "                # Per-eye validity using the validity columns\n",
    "                left_valid_col = 'left_gaze_origin_validity'\n",
    "                right_valid_col = 'right_gaze_origin_validity'\n",
    "                \n",
    "                left_valid_pct = (df[left_valid_col] == 1).sum() / total_rows * 100 if total_rows > 0 else 0\n",
    "                right_valid_pct = (df[right_valid_col] == 1).sum() / total_rows * 100 if total_rows > 0 else 0\n",
    "                \n",
    "                # Determine flags\n",
    "                flags = []\n",
    "                if sampling_rate < 80:\n",
    "                    flags.append(f\"LOW_SR({sampling_rate:.0f}Hz)\")\n",
    "                if valid_gaze_pct < 70:\n",
    "                    flags.append(f\"LOW_VALID({valid_gaze_pct:.1f}%)\")\n",
    "                \n",
    "                flag_str = \", \".join(flags) if flags else \"OK\"\n",
    "                \n",
    "                results.append({\n",
    "                    \"Group\": group_name,\n",
    "                    \"Participant\": pid,\n",
    "                    \"Phase\": phase,\n",
    "                    \"Status\": \"OK\",\n",
    "                    \"Total_Duration_s\": round(duration_s, 1),\n",
    "                    \"Total_Rows\": total_rows,\n",
    "                    \"Sampling_Rate_Hz\": round(sampling_rate, 1),\n",
    "                    \"Valid_Gaze_Pct\": round(valid_gaze_pct, 1),\n",
    "                    \"Left_Valid_Pct\": round(left_valid_pct, 1),\n",
    "                    \"Right_Valid_Pct\": round(right_valid_pct, 1),\n",
    "                    \"Flag\": flag_str\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                results.append({\n",
    "                    \"Group\": group_name,\n",
    "                    \"Participant\": pid,\n",
    "                    \"Phase\": phase,\n",
    "                    \"Status\": f\"ERROR: {str(e)[:50]}\",\n",
    "                    \"Total_Duration_s\": np.nan,\n",
    "                    \"Total_Rows\": 0,\n",
    "                    \"Sampling_Rate_Hz\": np.nan,\n",
    "                    \"Valid_Gaze_Pct\": np.nan,\n",
    "                    \"Left_Valid_Pct\": np.nan,\n",
    "                    \"Right_Valid_Pct\": np.nan,\n",
    "                    \"Flag\": \"ERROR\"\n",
    "                })\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "print(f\"Total entries: {len(df_results)}\")\n",
    "print(f\"Unique participants: {df_results['Participant'].nunique()}\")\n",
    "print(f\"Statuses: {df_results['Status'].value_counts().to_dict()}\")\n",
    "print(f\"Flags: {df_results['Flag'].value_counts().to_dict()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0dtryi0vph",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Let's check the timestamp unit by looking at consecutive differences\n",
    "import pandas as pd\n",
    "\n",
    "csv_path = \"/Users/kyoya/Laboratory/metacognition-analysis/data/input/A/P001/pre/eye_tracking/2026-01-16_17-33-45.563036/tobii_pro_gaze.csv\"\n",
    "df_sample = pd.read_csv(csv_path)\n",
    "\n",
    "ts = df_sample.iloc[:, 0].values\n",
    "print(\"First few timestamps:\", ts[:5])\n",
    "print(\"Diffs between consecutive timestamps (first 20):\", np.diff(ts[:21]))\n",
    "print(\"Mean diff:\", np.mean(np.diff(ts[:1000])))\n",
    "print(\"Median diff:\", np.median(np.diff(ts[:1000])))\n",
    "\n",
    "# If ~90Hz, the interval should be ~11.1ms or ~11111 microseconds\n",
    "# Diffs around 11 => ms\n",
    "# Diffs around 11000 => microseconds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "u0ksc3z2ehh",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Timestamps are in milliseconds! But mean diff is ~4696ms which is very high.\n",
    "# That suggests there are large gaps in the data. The median is ~11ms which is correct for 90Hz.\n",
    "# The large mean indicates gaps (e.g., between phases/screens).\n",
    "\n",
    "# Let me check - is this data for the ENTIRE session including idle time?\n",
    "# The duration calculated was total time from first to last timestamp\n",
    "# But the actual recording includes all screen transitions, reflections, etc.\n",
    "\n",
    "# So the issue is: the total duration includes ALL time (breaks, reflections, etc.)\n",
    "# but data is only recorded during active screen viewing.\n",
    "\n",
    "# Let me recalculate with a better approach:\n",
    "# 1. Use median inter-sample interval to determine actual sampling rate\n",
    "# 2. Count \"active\" samples (where inter-sample interval < some threshold)\n",
    "\n",
    "# For expected 90Hz: inter-sample should be ~11.1ms\n",
    "# Let's consider samples with gap < 50ms as \"continuous\"\n",
    "\n",
    "print(f\"Total rows: {len(df_sample)}\")\n",
    "print(f\"Total span: {(ts[-1] - ts[0])/1000:.1f} seconds = {(ts[-1] - ts[0])/1000/60:.1f} minutes\")\n",
    "\n",
    "diffs = np.diff(ts)\n",
    "print(f\"\\nInter-sample interval stats:\")\n",
    "print(f\"  Min: {np.min(diffs):.2f} ms\")\n",
    "print(f\"  Median: {np.median(diffs):.2f} ms\")\n",
    "print(f\"  Mean: {np.mean(diffs):.2f} ms\")\n",
    "print(f\"  Max: {np.max(diffs):.2f} ms\")\n",
    "print(f\"  Std: {np.std(diffs):.2f} ms\")\n",
    "\n",
    "# Percentage of \"normal\" intervals (< 15ms for 90Hz)\n",
    "normal_intervals = diffs < 15\n",
    "print(f\"\\n  Normal intervals (<15ms): {normal_intervals.sum()} ({normal_intervals.sum()/len(diffs)*100:.1f}%)\")\n",
    "print(f\"  Gap intervals (>=15ms): {(~normal_intervals).sum()}\")\n",
    "\n",
    "# Effective sampling rate from median\n",
    "effective_sr = 1000.0 / np.median(diffs)\n",
    "print(f\"\\n  Effective sampling rate (from median): {effective_sr:.1f} Hz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wpp1yyqjn5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "base_path = Path(\"/Users/kyoya/Laboratory/metacognition-analysis/data/input\")\n",
    "\n",
    "group_a = [\"P001\", \"P002\", \"P005\", \"P006\", \"P008\", \"P009\", \"P010\", \"P011\", \"P016\", \"P017\"]\n",
    "group_b = [\"P003\", \"P004\", \"P007\", \"P012\", \"P013\", \"P014\", \"P015\", \"P018\", \"P019\", \"P020\"]\n",
    "phases = [\"pre\", \"post\", \"training1\", \"training2\", \"training3\"]\n",
    "\n",
    "results = []\n",
    "\n",
    "for group_name, participants in [(\"A\", group_a), (\"B\", group_b)]:\n",
    "    for pid in participants:\n",
    "        for phase in phases:\n",
    "            eye_dir = base_path / group_name / pid / phase / \"eye_tracking\"\n",
    "            \n",
    "            if not eye_dir.exists():\n",
    "                results.append({\n",
    "                    \"Group\": group_name, \"Participant\": pid, \"Phase\": phase,\n",
    "                    \"Status\": \"MISSING_DIR\",\n",
    "                    \"Total_Span_min\": np.nan, \"Active_Duration_s\": np.nan,\n",
    "                    \"Total_Rows\": 0, \"Median_SR_Hz\": np.nan, \"Effective_SR_Hz\": np.nan,\n",
    "                    \"Valid_Gaze_Pct\": np.nan, \"Left_Valid_Pct\": np.nan, \"Right_Valid_Pct\": np.nan,\n",
    "                    \"Num_Gaps\": 0, \"Normal_Interval_Pct\": np.nan,\n",
    "                    \"Flag\": \"MISSING\"\n",
    "                })\n",
    "                continue\n",
    "            \n",
    "            subdirs = [d for d in eye_dir.iterdir() if d.is_dir()]\n",
    "            if len(subdirs) == 0:\n",
    "                results.append({\n",
    "                    \"Group\": group_name, \"Participant\": pid, \"Phase\": phase,\n",
    "                    \"Status\": \"NO_SUBDIR\",\n",
    "                    \"Total_Span_min\": np.nan, \"Active_Duration_s\": np.nan,\n",
    "                    \"Total_Rows\": 0, \"Median_SR_Hz\": np.nan, \"Effective_SR_Hz\": np.nan,\n",
    "                    \"Valid_Gaze_Pct\": np.nan, \"Left_Valid_Pct\": np.nan, \"Right_Valid_Pct\": np.nan,\n",
    "                    \"Num_Gaps\": 0, \"Normal_Interval_Pct\": np.nan,\n",
    "                    \"Flag\": \"MISSING\"\n",
    "                })\n",
    "                continue\n",
    "            \n",
    "            ts_dir = subdirs[0]\n",
    "            csv_path = ts_dir / \"tobii_pro_gaze.csv\"\n",
    "            \n",
    "            if not csv_path.exists():\n",
    "                results.append({\n",
    "                    \"Group\": group_name, \"Participant\": pid, \"Phase\": phase,\n",
    "                    \"Status\": \"NO_CSV\",\n",
    "                    \"Total_Span_min\": np.nan, \"Active_Duration_s\": np.nan,\n",
    "                    \"Total_Rows\": 0, \"Median_SR_Hz\": np.nan, \"Effective_SR_Hz\": np.nan,\n",
    "                    \"Valid_Gaze_Pct\": np.nan, \"Left_Valid_Pct\": np.nan, \"Right_Valid_Pct\": np.nan,\n",
    "                    \"Num_Gaps\": 0, \"Normal_Interval_Pct\": np.nan,\n",
    "                    \"Flag\": \"MISSING\"\n",
    "                })\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                df = pd.read_csv(csv_path)\n",
    "                ts_col = df.columns[0]\n",
    "                total_rows = len(df)\n",
    "                \n",
    "                if total_rows < 2:\n",
    "                    results.append({\n",
    "                        \"Group\": group_name, \"Participant\": pid, \"Phase\": phase,\n",
    "                        \"Status\": \"TOO_FEW_ROWS\",\n",
    "                        \"Total_Span_min\": np.nan, \"Active_Duration_s\": np.nan,\n",
    "                        \"Total_Rows\": total_rows, \"Median_SR_Hz\": np.nan, \"Effective_SR_Hz\": np.nan,\n",
    "                        \"Valid_Gaze_Pct\": np.nan, \"Left_Valid_Pct\": np.nan, \"Right_Valid_Pct\": np.nan,\n",
    "                        \"Num_Gaps\": 0, \"Normal_Interval_Pct\": np.nan,\n",
    "                        \"Flag\": \"TOO_FEW_ROWS\"\n",
    "                    })\n",
    "                    continue\n",
    "                \n",
    "                timestamps = df[ts_col].values\n",
    "                total_span_s = (timestamps[-1] - timestamps[0]) / 1000.0\n",
    "                total_span_min = total_span_s / 60.0\n",
    "                \n",
    "                diffs = np.diff(timestamps)\n",
    "                \n",
    "                # Normal intervals: < 15ms (expected ~11ms for 90Hz)\n",
    "                GAP_THRESHOLD_MS = 15\n",
    "                normal_mask = diffs < GAP_THRESHOLD_MS\n",
    "                normal_intervals = diffs[normal_mask]\n",
    "                gap_intervals = diffs[~normal_mask]\n",
    "                \n",
    "                normal_interval_pct = normal_mask.sum() / len(diffs) * 100\n",
    "                num_gaps = (~normal_mask).sum()\n",
    "                \n",
    "                # Median sampling rate from normal intervals\n",
    "                if len(normal_intervals) > 0:\n",
    "                    median_sr = 1000.0 / np.median(normal_intervals)\n",
    "                else:\n",
    "                    median_sr = 0\n",
    "                \n",
    "                # Active duration: sum of normal intervals\n",
    "                active_duration_s = normal_intervals.sum() / 1000.0\n",
    "                \n",
    "                # Effective sampling rate: rows / total span\n",
    "                effective_sr = total_rows / total_span_s if total_span_s > 0 else 0\n",
    "                \n",
    "                # Valid gaze percentage\n",
    "                valid_gaze = df['gaze_x'].notna() & df['gaze_y'].notna()\n",
    "                valid_gaze_pct = valid_gaze.sum() / total_rows * 100\n",
    "                \n",
    "                # Per-eye validity\n",
    "                left_valid_pct = (df['left_gaze_origin_validity'] == 1).sum() / total_rows * 100\n",
    "                right_valid_pct = (df['right_gaze_origin_validity'] == 1).sum() / total_rows * 100\n",
    "                \n",
    "                # Flags\n",
    "                flags = []\n",
    "                if median_sr < 85:\n",
    "                    flags.append(f\"LOW_MEDIAN_SR({median_sr:.0f}Hz)\")\n",
    "                if valid_gaze_pct < 70:\n",
    "                    flags.append(f\"LOW_VALID({valid_gaze_pct:.1f}%)\")\n",
    "                if left_valid_pct < 70 and right_valid_pct < 70:\n",
    "                    flags.append(f\"BOTH_EYES_LOW\")\n",
    "                elif left_valid_pct < 70:\n",
    "                    flags.append(f\"LEFT_EYE_LOW({left_valid_pct:.0f}%)\")\n",
    "                elif right_valid_pct < 70:\n",
    "                    flags.append(f\"RIGHT_EYE_LOW({right_valid_pct:.0f}%)\")\n",
    "                \n",
    "                flag_str = \", \".join(flags) if flags else \"OK\"\n",
    "                \n",
    "                results.append({\n",
    "                    \"Group\": group_name,\n",
    "                    \"Participant\": pid,\n",
    "                    \"Phase\": phase,\n",
    "                    \"Status\": \"OK\",\n",
    "                    \"Total_Span_min\": round(total_span_min, 1),\n",
    "                    \"Active_Duration_s\": round(active_duration_s, 1),\n",
    "                    \"Total_Rows\": total_rows,\n",
    "                    \"Median_SR_Hz\": round(median_sr, 1),\n",
    "                    \"Effective_SR_Hz\": round(effective_sr, 1),\n",
    "                    \"Valid_Gaze_Pct\": round(valid_gaze_pct, 1),\n",
    "                    \"Left_Valid_Pct\": round(left_valid_pct, 1),\n",
    "                    \"Right_Valid_Pct\": round(right_valid_pct, 1),\n",
    "                    \"Num_Gaps\": int(num_gaps),\n",
    "                    \"Normal_Interval_Pct\": round(normal_interval_pct, 1),\n",
    "                    \"Flag\": flag_str\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                results.append({\n",
    "                    \"Group\": group_name, \"Participant\": pid, \"Phase\": phase,\n",
    "                    \"Status\": f\"ERROR: {str(e)[:60]}\",\n",
    "                    \"Total_Span_min\": np.nan, \"Active_Duration_s\": np.nan,\n",
    "                    \"Total_Rows\": 0, \"Median_SR_Hz\": np.nan, \"Effective_SR_Hz\": np.nan,\n",
    "                    \"Valid_Gaze_Pct\": np.nan, \"Left_Valid_Pct\": np.nan, \"Right_Valid_Pct\": np.nan,\n",
    "                    \"Num_Gaps\": 0, \"Normal_Interval_Pct\": np.nan,\n",
    "                    \"Flag\": \"ERROR\"\n",
    "                })\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "print(f\"Processing complete: {len(df_results)} entries\")\n",
    "print(f\"All statuses: {df_results['Status'].value_counts().to_dict()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fsfbbixlf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Now let's display the comprehensive results table\n",
    "\n",
    "# Sort: problematic first (non-OK flags), then by group/participant/phase\n",
    "df_results['is_flagged'] = df_results['Flag'] != 'OK'\n",
    "df_results_sorted = df_results.sort_values(\n",
    "    by=['is_flagged', 'Valid_Gaze_Pct', 'Group', 'Participant', 'Phase'],\n",
    "    ascending=[False, True, True, True, True]\n",
    ").drop(columns=['is_flagged'])\n",
    "\n",
    "# Set display options for full table\n",
    "pd.set_option('display.max_rows', 120)\n",
    "pd.set_option('display.max_columns', 20)\n",
    "pd.set_option('display.width', 250)\n",
    "pd.set_option('display.max_colwidth', 40)\n",
    "\n",
    "print(\"=\" * 220)\n",
    "print(\"EYE TRACKING DATA QUALITY REPORT - ALL PARTICIPANTS & PHASES\")\n",
    "print(\"=\" * 220)\n",
    "print()\n",
    "\n",
    "# First show flagged entries\n",
    "flagged = df_results_sorted[df_results_sorted['Flag'] != 'OK']\n",
    "print(f\"*** FLAGGED ENTRIES: {len(flagged)} out of {len(df_results)} ***\")\n",
    "print()\n",
    "\n",
    "if len(flagged) > 0:\n",
    "    print(flagged[['Group', 'Participant', 'Phase', 'Total_Span_min', 'Active_Duration_s', \n",
    "                    'Total_Rows', 'Median_SR_Hz', 'Valid_Gaze_Pct', 'Left_Valid_Pct', \n",
    "                    'Right_Valid_Pct', 'Num_Gaps', 'Flag']].to_string(index=False))\n",
    "    print()\n",
    "\n",
    "print(\"-\" * 220)\n",
    "print()\n",
    "print(\"FULL TABLE (sorted: flagged first, then by valid gaze %)\")\n",
    "print()\n",
    "print(df_results_sorted[['Group', 'Participant', 'Phase', 'Total_Span_min', 'Active_Duration_s',\n",
    "                          'Total_Rows', 'Median_SR_Hz', 'Valid_Gaze_Pct', 'Left_Valid_Pct',\n",
    "                          'Right_Valid_Pct', 'Num_Gaps', 'Flag']].to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vv1gu13ahc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Now let's print summary statistics\n",
    "\n",
    "print(\"=\" * 120)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\" * 120)\n",
    "print()\n",
    "\n",
    "ok_data = df_results[df_results['Status'] == 'OK']\n",
    "\n",
    "print(\"--- Overall ---\")\n",
    "print(f\"  Total participant-phase combinations: {len(df_results)}\")\n",
    "print(f\"  Successfully loaded: {len(ok_data)}\")\n",
    "print(f\"  Flagged (any issue): {len(ok_data[ok_data['Flag'] != 'OK'])}\")\n",
    "print()\n",
    "\n",
    "print(\"--- Median Sampling Rate (Hz) ---\")\n",
    "print(f\"  Mean:   {ok_data['Median_SR_Hz'].mean():.1f}\")\n",
    "print(f\"  Median: {ok_data['Median_SR_Hz'].median():.1f}\")\n",
    "print(f\"  Min:    {ok_data['Median_SR_Hz'].min():.1f}\")\n",
    "print(f\"  Max:    {ok_data['Median_SR_Hz'].max():.1f}\")\n",
    "print()\n",
    "\n",
    "print(\"--- Valid Gaze Percentage ---\")\n",
    "print(f\"  Mean:   {ok_data['Valid_Gaze_Pct'].mean():.1f}%\")\n",
    "print(f\"  Median: {ok_data['Valid_Gaze_Pct'].median():.1f}%\")\n",
    "print(f\"  Min:    {ok_data['Valid_Gaze_Pct'].min():.1f}%\")\n",
    "print(f\"  Max:    {ok_data['Valid_Gaze_Pct'].max():.1f}%\")\n",
    "print(f\"  <70%:   {len(ok_data[ok_data['Valid_Gaze_Pct'] < 70])} entries\")\n",
    "print(f\"  <80%:   {len(ok_data[ok_data['Valid_Gaze_Pct'] < 80])} entries\")\n",
    "print(f\"  <90%:   {len(ok_data[ok_data['Valid_Gaze_Pct'] < 90])} entries\")\n",
    "print()\n",
    "\n",
    "print(\"--- Per-Group Summary ---\")\n",
    "for grp in ['A', 'B']:\n",
    "    grp_data = ok_data[ok_data['Group'] == grp]\n",
    "    print(f\"\\n  Group {grp}:\")\n",
    "    print(f\"    Mean Valid Gaze:   {grp_data['Valid_Gaze_Pct'].mean():.1f}%\")\n",
    "    print(f\"    Median Valid Gaze: {grp_data['Valid_Gaze_Pct'].median():.1f}%\")\n",
    "    print(f\"    Mean Median SR:    {grp_data['Median_SR_Hz'].mean():.1f} Hz\")\n",
    "    print(f\"    Flagged entries:   {len(grp_data[grp_data['Flag'] != 'OK'])}\")\n",
    "\n",
    "print()\n",
    "print(\"--- Per-Phase Summary ---\")\n",
    "for phase in phases:\n",
    "    ph_data = ok_data[ok_data['Phase'] == phase]\n",
    "    print(f\"\\n  {phase}:\")\n",
    "    print(f\"    Mean Valid Gaze:   {ph_data['Valid_Gaze_Pct'].mean():.1f}%\")\n",
    "    print(f\"    Median Valid Gaze: {ph_data['Valid_Gaze_Pct'].median():.1f}%\")\n",
    "    print(f\"    Flagged entries:   {len(ph_data[ph_data['Flag'] != 'OK'])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hduc8qx2nav",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Let's also look at per-participant averages to identify systematically problematic participants\n",
    "\n",
    "print(\"=\" * 120)\n",
    "print(\"PER-PARTICIPANT SUMMARY (averaged across all phases)\")\n",
    "print(\"=\" * 120)\n",
    "print()\n",
    "\n",
    "participant_summary = ok_data.groupby(['Group', 'Participant']).agg({\n",
    "    'Valid_Gaze_Pct': ['mean', 'min', 'std'],\n",
    "    'Left_Valid_Pct': 'mean',\n",
    "    'Right_Valid_Pct': 'mean',\n",
    "    'Total_Rows': 'sum',\n",
    "    'Active_Duration_s': 'sum',\n",
    "    'Num_Gaps': 'sum'\n",
    "}).round(1)\n",
    "\n",
    "participant_summary.columns = ['Valid_Gaze_Mean', 'Valid_Gaze_Min', 'Valid_Gaze_Std',\n",
    "                                'Left_Valid_Mean', 'Right_Valid_Mean',\n",
    "                                'Total_Rows_All', 'Active_Duration_Total_s', 'Total_Gaps']\n",
    "\n",
    "# Add a note column\n",
    "participant_summary['Note'] = ''\n",
    "for idx in participant_summary.index:\n",
    "    notes = []\n",
    "    if participant_summary.loc[idx, 'Valid_Gaze_Mean'] < 85:\n",
    "        notes.append('LOW_AVG_VALID')\n",
    "    if participant_summary.loc[idx, 'Valid_Gaze_Min'] < 70:\n",
    "        notes.append('HAS_LOW_PHASE')\n",
    "    if participant_summary.loc[idx, 'Valid_Gaze_Std'] > 15:\n",
    "        notes.append('HIGH_VARIABILITY')\n",
    "    participant_summary.loc[idx, 'Note'] = ', '.join(notes) if notes else 'OK'\n",
    "\n",
    "participant_summary = participant_summary.sort_values('Valid_Gaze_Mean', ascending=True)\n",
    "print(participant_summary.to_string())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-science-intro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
